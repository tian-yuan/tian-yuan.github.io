{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2017-12-11T11:49:30.048Z","updated":"2017-12-11T11:49:30.048Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2017-12-11T11:47:19.989Z","updated":"2017-12-11T11:47:19.989Z","comments":false,"path":"books/index.html","permalink":"http://yoursite.com/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2017-12-11T11:47:06.030Z","updated":"2017-12-11T11:47:06.029Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2017-12-11T11:37:14.489Z","updated":"2017-12-11T11:37:14.488Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"理解MQTT QOS","slug":"理解MQTT-QOS","date":"2018-01-10T02:38:20.000Z","updated":"2018-01-10T03:20:57.742Z","comments":true,"path":"2018/01/10/理解MQTT-QOS/","link":"","permalink":"http://yoursite.com/2018/01/10/理解MQTT-QOS/","excerpt":"","text":"理解 MQTT QOS简介MQTT 提供了 3 种 QOS 级别，分别是 QOS0、QOS1、QOS2； 本文档分别介绍这三种 QOS，请注意 PUBLISH 侧的 QOS 和 SUBSCRIBE 侧的 QOS 在流程上的区别，以及 PUBLISH QOS 和 SUBSCRIBE QOS 组合后的服务 QOS； QOS0—最多发送一次这种 QOS 是发布速度最快，效率最高的，但是同时也是最不可靠的一种传输模式； 以 QOS0 发布的消息不会存储在发送端，同时发布后没有 ACK；最多发送一次；只要消息从发送端发送出去，消息就会从发送端的出队列里面剔除；因此这种 QOS 级别的发布不存在消息重复的情况； QOS1—至少发送一次 这种级别的消息发布，可以保证消息至少被发布一次，有可能被发布多次。 发布 QOS 为 1 的消息，需要两次消息交互；发送方发送一个消息，然后等待接收方的回复（PUBACK）。 如果发送方接收到 ACK 消息，客户端 APP 就把发布消息从发送队列里面清除； 如果发送方没有接收到 ACK 消息，客户端会重新发送消息，同时设置消息的 DUP 标记。消息会被以指定时间间隔重新发送直到发送方接收到 ACK。如果 broker 接收到消息，broker 就把消息转发给消息订阅者，即使消息存在 DUP 标记，因此消息订阅者有可能多次接收到同一条消息。 QOS2—只发送一次 这种消息发布级别的服务是效率最低的，需要进行四次消息交互才能完成一条消息的发布； 发送者发布一条消息，等待接收回复（PUBREC） 接收者回复一条 PUBREC 消息 如果发送者没有接收到 PUBREC 回应消息，发送者会重发消息，同时设置消息的 DUB 标记 当发送者接收到 PUBREC 消息后，发送者发送 PUBREL 消息给接收者 如果接收者没有接收到 PUBREL 消息，接收者继续回复 PUBREC 消息给发送者 当接收者接收到 PUBREL 消息后，接收者转发消息到消息订阅者 接收者接收到 PUBREL 消息后，同时回复 PUBCOMP 消息给发送者 当发送者接收到 PUBCOMP 消息后，发送者把发布消息从发送队列中清理掉 如果发送者没有接收到 PUBCOMP 消息，发送者会继续发送 PUBREL 消息； 相关 example 可以参照：http://www.steves-internet-guide.com/understanding-mqtt-qos-2/ 客户端到客户端 QOS两个连接到相同 broker 上的客户端之间的服务质量是由 PUBLISH 消息的 QOS 和 SUBSCRIBE 的 QOS 共同决定的。 整个系统 QOS 总是取决于 PUBLISH 和 SUBSCRIBE QOS 两者的最小值，可以参见下表： QOS PUBLISH QOS SUBSCRIBE OVERALL QOS 0 0 or 1 or 2 0 1 0 0 1 1 or 2 1 2 0 0 2 1 1 2 2 2","categories":[],"tags":[{"name":"IOT","slug":"IOT","permalink":"http://yoursite.com/tags/IOT/"}]},{"title":"socat tun","slug":"socat-tun","date":"2018-01-04T02:26:39.000Z","updated":"2018-01-04T02:28:29.775Z","comments":true,"path":"2018/01/04/socat-tun/","link":"","permalink":"http://yoursite.com/2018/01/04/socat-tun/","excerpt":"","text":"【转载】使用 socat 和 kcptun 实现 vpn 转载自 https://imkira.com/a18.html 工具简介socat 是一个流重定向工具，例如可以将一个文件流通过 tcp 发送出去。本文主要使用 socat 的 tun 转发功能。socat 可以使用如下命令安装： 12yum install -y socat kcptun 是一个使用 udp 代替 tcp 的数据承载工具。在较差的网络环境中可以用带宽换时延，提高网络速度。kcptun 是一个基于 kcp 协议的开源 tunnel 项目，可以在 Github 下载，注意选择系统版本 。 工作原理 socat 创建 tun 设备，并读取 tun 设备的所有 ip 数据包，将数据包发送至另一端。 原则上 socat 客户端与 socat 服务端之间可以不使用 kcptun ， socat 客户端可以直接与 socat 服务端通信。 使用 kcptun 的目的是提高客户端与服务端之间的通信效率。 配置服务端：1234567# 启动 socat ，监听 tcp 5001 端口，设置 tun 设备 ip 地址为 10.0.0.1/24nohup socat -d -d TCP-LISTEN:5001,reuseaddr TUN:10.0.0.1/24,up &gt; socat.out 2&gt;&amp;1 &amp;# 启动 kcptun ，监听 udp 5000 端口， 把数据都转发给 127.0.0.1:5001# 此处 kcptun 使用的是 linux/amd64 版本nohup ./server_linux_amd64 -t 127.0.0.1:5001 -l :5000 -mode fast2 &amp; &gt; kcptun.out 2&gt;&amp;1 &amp; 增加 iptables 规则： 1234# eth0 是服务器的外网网卡，根据服务器的网卡情况设置# 这条规则用于伪装 tun 的数据包iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -o eth0 -j MASQUERADE 客户端：123456# 启动 kcptun ， 监听 tcp 5001 端口， 把数据都转发给 服务器IP:5000 nohup ./client_linux_amd64 -r 服务器IP:5000 -l :5001 -mode fast2 &gt; kcptun.out 2&gt;&amp;1 &amp;# 启动 socat ，将 tun 设备所有数据包发送至 127.0.0.1:5001nohup socat TCP:127.0.0.1:5001 TUN:10.0.0.2/24,up &gt; socat.out 2&gt;&amp;1 &amp; 增加 iptables 规则： 1234# tun0 是 socat 创建的 tun 网卡，需要根据实际情况设置# 这条规则用于伪装 tun 的数据包iptables -t nat -A POSTROUTING -o tun0 -j MASQUERADE 客户端路由设置如果没有设置客户端的路由，那么只能访问 10.0.0.1 。ping 10.0.0.1 的情况: 1234567891011ping -I tun0 10.0.0.1PING 10.0.0.1 (10.0.0.1) from 10.0.0.2 tun0: 56(84) bytes of data.64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=276 ms64 bytes from 10.0.0.1: icmp_seq=2 ttl=64 time=272 ms64 bytes from 10.0.0.1: icmp_seq=3 ttl=64 time=278 ms64 bytes from 10.0.0.1: icmp_seq=4 ttl=64 time=270 ms--- 10.0.0.1 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3003msrtt min/avg/max/mdev = 270.519/274.514/278.211/3.022 ms 这个 IP 是服务端的 tun0 的 IP 地址，客户端的 socat 在创建 tun0 的同时会加上一个对应 10.0.0.0/24 的路由，所以 10.0.0.1 能够正常访问。路由如下： 1210.0.0.0/24 dev tun0 proto kernel scope link src 10.0.0.2 ping 8.8.8.8 的情况： 123456ping -I tun0 8.8.8.8PING 8.8.8.8 (8.8.8.8) from 10.0.0.2 tun0: 56(84) bytes of data.--- 8.8.8.8 ping statistics ---6 packets transmitted, 0 received, 100% packet loss, time 5000ms 在客户端通过 tcpdump 确认 tun0 能正常的从 socat 发出并收到服务端的 IP 报文。ip -s addr show tun0 的 RX 和 TX 显示 IP 报文正常接收，没有错误，也没有 drop，但是无法被发出 IP 报文的应用程序接收。 添加对应 IP 的路由后，即可正常访问这些 IP ： 123# 8.8.0.0/16 网段的 ip 都会通过 tun0 转发，根据实际需要设置路由即可ip route add 8.8.0.0/16 dev tun0","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"RIP Protocol","slug":"RIP-Protocol","date":"2017-12-25T11:16:33.000Z","updated":"2017-12-26T05:41:33.486Z","comments":true,"path":"2017/12/25/RIP-Protocol/","link":"","permalink":"http://yoursite.com/2017/12/25/RIP-Protocol/","excerpt":"","text":"RIP 协议介绍​ RIP是Routing Information Protocol（路由信息协议）的简称，它是一种较为简单的内部网关协议（Interior Gateway Protocol）。 ​ RIP是一种基于距离矢量（Distance-Vector）算法的协议，它使用跳数（Hop Count）作为度量来衡量到达目的网络的距离。设备到与他直连网络的设备跳数为0，然后每经过一个三层设备跳数增加1，也就是度量值等于从本网络到达目网络间的三层设备数量，但并不等于所经过的网段数RIP通过UDP报文进行路由信息的交换，使用的端口号为520。所以它又是一个不可靠的路由协议。由于RIP的实现较为简单，在配置和维护管理方面也远比OSPF和IS-IS容易，因此RIP主要应用于规模较小的网络中，例如校园网以及结构较简单的地区性网络。对于更为复杂的环境和大型网络，一般不使用RIP协议。 RIP 协议原理​ 每隔30秒会与相邻的路由器交换子讯息，以动态的建立路由表。 RIP 封装格式1234567891011121314150 1 2 30 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| command (1) | version (1) | must be zero (2) |+---------------+---------------+-------------------------------+| address family identifier (2) | must be zero (2) |+-------------------------------+-------------------------------+| IP address (4) |+---------------------------------------------------------------+| must be zero (4) |+---------------------------------------------------------------+| must be zero (4) |+---------------------------------------------------------------+| metric (4) |+---------------------------------------------------------------+ RIP规定度量值取0~15之间的整数，大于或等于16的跳数被定义为无穷大。 版本目前RIP共有三个版本，RIPv1、RIPv2、RIPng。 RIPv1RIPv1使用分类路由，定义在《RFC 1058》中。在它的路由更新（Routing Updates）中并不带有子网的资讯，因此它无法支援可变长度子网掩码。这个限制造成在RIPv1的网络中，在同级网络下无法使用不同的子网掩码。换句话说，在同一个网络下所有的子网络数目都是相同的。另外，RIPv1的协议报文中没有验证字段，所以RIPv1不支持验证。 RIPv1是一个基于UDP的路由协议，并且RIPv1的数据包不能超过512字节（RIP报文头部占用4个字节,而每个路由条目占用20个八位组字节。因此,RIP消息最大为4+(25*20)=504个字节,再加上8个字节的UDP头部,所以RIP数据报的大小(不含IP包的头部)最大可达512个字节。）。 RIPv2RIPv2在RIPv1的基础上改进如下几点： 支持外部路由标记（Route Tag），可以在路由策略中根据Tag对路由进行灵活的控制。实际上不同RIP进程间相互引入路由也可以使用Tag。 报文中携带掩码信息，支持路由聚合和CIDR。 支持指定下一跳，在广播网上可以选择到目的网段最优下一跳地址。 支持以组播方式发送更新报文，只有运行RIPv2的设备才能收到协议报文，减少资源消耗。 支持对协议报文进行验证，增强安全性。 RIPngRIPng（Routing Information Protocol next generation）在RFC 2080中被定义，主要是针对IPv6做一些延伸的规范。与RIPv2相比下其最主要的差异是： RIPv2 支援RIP更新认证, RIPng 则不支持，因为IPv6路由器理应会使用IPsec来进行身份验证; RIPv2 容许给路由器附上任何标签， RIPng 则不容许； RIPv2 在每个路由表项中都保存下一跳的信息，RIPng 是对一组路由表项指定下一跳信息； RIPv2 使用UDP端口520和多播地址224.0.0.9通信，RIPng 则使用UDP端口521和多播地址FF02::9通信 RIP协议定时器RIP在更新和维护路由信息时主要使用以下4个定时器： 更新定时器：当此超时器超时时，立即发送路由更新报文，缺省每30s发送一次。 老化定时器：RIP设备如果在老化时间内没有收到邻居发来的路由更新报文，则认为该路由不可达。当学到一条路由并添加到RIP路由表中时，老化定时器启动，如果老化定时器超时，设备仍没有收到邻居发来的更新报文，则吧该路由的度量值置为16，并启动垃圾收集定时器。 垃圾收集定时器：如果在垃圾收集时间内仍没有收到原来不可达到路由的更新，该路由将被从RIP路由表彻底删除。 抑制定时器：当RIP设备收到对端的路由更新，其度量值为16。则对应路由进入抑制状态，并启动抑制定时器，缺省值为180s。这时，为了防止路由震荡，在抑制定时器超时之前，即使再收到对端路由度量值小于16的更新，也不接受。当抑制超时器超时后，就重新接受对端发送的路由更新报文。 RIP 原理演示 上述网络拓扑图包含三个路由器，每个路由器链接两个不同的网段； 刚开始三个路由器的路由表信息如下（以R1作为实例）： 目的网络 下一跳 距离 10.0.0.0 — 0 20.0.0.0 — 0 经过定时器定时更新，下一个周期的路由表信息如下（以R1作为实例）： 目的网络 下一跳 距离 10.0.0.0 — 0 20.0.0.0 — 0 30.0.0.0 20.0.0.9 1 40.0.0.0 20.0.0.9 2 由于 RIP 有跳数限制，就出现如下的扩大跳数限制的方案： 配置GRE扩大跳数受限的网络工作范围示例​ 如下图所示，RouterA、RouterB、RouterC和RouterD之间需要部署RIP路由协议实现互通，要求RouterA到RouterD只经过1跳，即Cost值为1。不改变组网的情况下，正常部署RIP协议，RouterA到RouterD需要经过两台设备，跳数为2，此时可以在RouterA和RouterC之间部署GRE隧道，隐藏中间设备RouterB，实现RouterA到RouterD的跳数为1，可以看出，其经过的跳数实际为2，相当于扩大了RIP协议的跳数受限的网络工作范围。 配置思路 1234567配置GRE扩大跳数受限的网络工作范围的思路如下：在设备RouterA、RouterB和RouterC上运行RIP协议，进程为1，实现互通。RouterA与RouterC之间建立GRE隧道，隐藏RouterB。在设备RouterA、RouterC和RouterD上运行RIP协议，进程为2，使RIP路由经过GRE隧道传输，扩大了RIP协议实际的跳数范围。 操作步骤 配置各物理接口IP地址 12345678910111213141516171819202122232425262728293031323334# 配置RouterA。&lt;Huawei&gt; system-view[Huawei] sysname RouterA[RouterA] interface gigabitethernet 1/0/0[RouterA-GigabitEthernet1/0/0] ip address 20.1.1.1 255.255.255.0[RouterA-GigabitEthernet1/0/0] quit# 配置RouterB。&lt;Huawei&gt; system-view[Huawei] sysname RouterB[RouterB] interface gigabitethernet 1/0/0[RouterB-GigabitEthernet1/0/0] ip address 20.1.1.2 255.255.255.0[RouterB-GigabitEthernet1/0/0] quit[RouterB] interface gigabitethernet 2/0/0[RouterB-GigabitEthernet2/0/0] ip address 30.1.1.1 255.255.255.0[RouterB-GigabitEthernet2/0/0] quit# 配置RouterC。&lt;Huawei&gt; system-view[Huawei] sysname RouterC[RouterC] interface gigabitethernet 1/0/0[RouterC-GigabitEthernet1/0/0] ip address 30.1.1.2 255.255.255.0[RouterC-GigabitEthernet1/0/0] quit[RouterC] interface gigabitethernet 2/0/0[RouterC-GigabitEthernet2/0/0] ip address 40.1.1.1 255.255.255.0[RouterC-GigabitEthernet2/0/0] quit# 配置RouterD。&lt;Huawei&gt; system-view[Huawei] sysname RouterD[RouterD] interface gigabitethernet 1/0/0[RouterD-GigabitEthernet1/0/0] ip address 40.1.1.2 255.255.255.0[RouterD-GigabitEthernet1/0/0] quit 配置设备间使用RIP路由，进程为1 123456789101112131415161718192021222324252627282930313233343536373839# 配置RouterA。[RouterA] rip 1[RouterA-rip-1] version 2[RouterA-rip-1] network 20.0.0.0 [RouterA-rip-1] quit# 配置RouterB。[RouterB] rip 1[RouterB-rip-1] version 2[RouterB-rip-1] network 20.0.0.0[RouterB-rip-1] network 30.0.0.0[RouterB-rip-1] quit# 配置RouterC。[RouterC] rip 1[RouterC-rip-1] version 2[RouterC-rip-1] network 30.0.0.0[RouterC-rip-1] quit# 配置完成后，在RouterA和RouterC上执行display ip routing-table命令，可以看到它们能够学到去往对端接口网段地址的RIP路由。# 以RouterA的显示为例。[RouterA] display ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 8 Routes : 8Destination/Mask Proto Pre Cost Flags NextHop Interface 20.1.1.0/24 Direct 0 0 D 20.1.1.1 GigabitEthernet1/0/0 20.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet1/0/0 20.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet1/0/0 30.1.1.0/24 RIP 100 1 D 20.1.1.2 GigabitEthernet1/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 配置Tunnel接口 123456789101112131415161718192021222324252627282930313233# 配置RouterA。[RouterA] interface tunnel 0/0/1[RouterA-Tunnel0/0/1] tunnel-protocol gre[RouterA-Tunnel0/0/1] ip address 50.1.1.1 255.255.255.0[RouterA-Tunnel0/0/1] source 20.1.1.1[RouterA-Tunnel0/0/1] destination 30.1.1.2[RouterA-Tunnel0/0/1] quit# 配置RouterC。[RouterC] interface tunnel 0/0/1[RouterC-Tunnel0/0/1] tunnel-protocol gre[RouterC-Tunnel0/0/1] ip address 50.1.1.2 255.255.255.0[RouterC-Tunnel0/0/1] source 30.1.1.2[RouterC-Tunnel0/0/1] destination 20.1.1.1[RouterC-Tunnel0/0/1] quit# 配置完成后，Tunnel接口状态变为Up，Tunnel接口之间可以Ping通。# 以RouterA的显示为例：[RouterA] ping -a 50.1.1.1 50.1.1.2 PING 50.1.1.2: 56 data bytes, press CTRL_C to break Reply from 50.1.1.2: bytes=56 Sequence=1 ttl=255 time=1 ms Reply from 50.1.1.2: bytes=56 Sequence=2 ttl=255 time=1 ms Reply from 50.1.1.2: bytes=56 Sequence=3 ttl=255 time=1 ms Reply from 50.1.1.2: bytes=56 Sequence=4 ttl=255 time=1 ms Reply from 50.1.1.2: bytes=56 Sequence=5 ttl=255 time=1 ms --- 50.1.1.2 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 1/1/1 ms 配置Tunnel接口使用RIP路由，进程为2 12345678910111213141516171819# 配置RouterA。[RouterA] rip 2[RouterA-rip-2] version 2[RouterA-rip-2] network 50.0.0.0[RouterA-rip-2] quit# 配置RouterC。[RouterC] rip 2[RouterC-rip-2] version 2[RouterC-rip-2] network 50.0.0.0[RouterC-rip-2] network 40.0.0.0[RouterC-rip-2] quit# 配置RouterD。[RouterD] rip 2[RouterD-rip-2] version 2[RouterD-rip-2] network 40.0.0.0[RouterD-rip-2] quit ​ 检查配置结果 123456789101112131415161718192021222324# 配置完成后，在RouterA和RouterD上执行display ip routing-table命令，可以看到目的地址为对端设备的路由，其Cost值为1。# 以RouterA的显示为例。[RouterA] display ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 12 Routes : 12Destination/Mask Proto Pre Cost Flags NextHop Interface 20.1.1.0/24 Direct 0 0 D 20.1.1.1 GigabitEthernet1/0/0 20.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet1/0/0 20.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet1/0/0 30.1.1.0/24 RIP 100 1 D 20.1.1.2 GigabitEthernet1/0/0 40.1.1.0/24 RIP 100 1 D 50.1.1.2 Tunnel0/0/1 50.1.1.0/24 Direct 0 0 D 50.1.1.1 Tunnel0/0/1 50.1.1.1/32 Direct 0 0 D 127.0.0.1 Tunnel0/0/1 50.1.1.255/32 Direct 0 0 D 127.0.0.1 Tunnel0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 注意：上述示例是参照华为 GRE 配置文档","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"Https Certification Erro","slug":"Https-Certification-Erro","date":"2017-12-22T03:42:58.000Z","updated":"2017-12-25T07:16:52.081Z","comments":true,"path":"2017/12/22/Https-Certification-Erro/","link":"","permalink":"http://yoursite.com/2017/12/22/Https-Certification-Erro/","excerpt":"","text":"简介此文档记录使用 HTTPS 证书过程中出现的问题； Unable to Connect to SSL Services due to PKIX Path Building Failed使用 java 代码链接 https 网站时，出现的问题； CauseWhenever Java attempts to connect to another application over SSL (e.g.: HTTPS, IMAPS, LDAPS), it will only be able to connect to that application if it can trust it. The way trust is handled in the Java world is that you have a keystore (typically $JAVA_HOME/lib/security/cacerts), also known as the truststore. This contains a list of all known Certificate Authority (CA) certificates, and Java will only trust certificates that are signed by one of those CAs or public certificates that exist within that keystore. For example, if we look at the certificate for Atlassian, we can see that the *.atlassian.com certificate has been signed by the intermediate certificates, DigiCert High Assurance EV Root CA and DigiCert High Assurance CA-3. These intermediate certificates have been signed by the root Entrust.net Secure Server CA: These three certificates combined are referred to as the certificate chain, and, as they are all within the Java keystore (cacerts), Java will trust any certificates signed by them (in this case, *.atlassian.com). Alternatively, if the *.atlassian.com certificate had been in the keystore, Java would also trust that site. This problem is therefore caused by a certificate that is self-signed (a CA did not sign it) or a certificate chain that does not exist within the Java truststore. Java does not trust the certificate and fails to connect to the application. Resolution Make sure you have imported the public certificate of the target instance into the truststore according to the Connecting to SSL Services instructions. Make sure any certificates have been imported into the correct truststore; you may have multiple JRE/JDKs. See Installing Java for this. Check to see that the correct truststore is in use. If -Djavax.net.ssl.trustStorehas been configured, it will override the location of the default truststore, which will need to be checked. Check if your Anti Virus tool has “SSL Scanning” blocking SSL/TLS. If it does, disable this feature or set exceptions for the target addresses (check the product documentation to see if this is possible.) If connecting to a mail server, such as Exchange, ensure authentication allows plain text. Verify that the target server is configured to serve SSL correctly. This can be done with the SSL Server Test tool. If all else fails, your truststore might be out of date. Upgrade Java to the latest version supported by your application. SolvedWe experienced this issue when a server changed their HTTPS SSL certificate, and our older version of Java did not recognize the root certificate authority (CA). If you can access the HTTPS URL in your browser then it is possible to update Java to recognize the root CA. In your browser, go to the HTTPS URL that Java could not access. Click on the HTTPS certificate chain (there is lock icon in the Internet Explorer, or the domain name left of the URL in firefox) and navigate the certificate hierarchy. At the top there should be a Primary Root CA. This could be missing from your java cacerts file. Note down the Issuer and Serial Number. To verify the root certificates, determine where the cacerts file is located. By default it is injre/lib/security/cacerts. The default password for this keystore is ‘changeit’. e.g. on my machine, I have both JDK and JRE, here is where they are located. 12./jdk1.6.0_24/jre/lib/security/cacerts./jre1.6.0_24/lib/security/cacerts Different versions of java can have different cacerts. If you do not want to modify the default JRE store, you can make a copy, and use the following system properties to specify the location and password. 12javax.net.ssl.trustStorejavax.net.ssl.trustStorePassword Once you have your keystore, dump its contents by using the list option. 12keytool -list -v -keystore /path/to/cacerts &gt; java_cacerts.txtEnter keystore password: changeit In this example, /path/to/cacerts is the location of your cacerts file, and the output of the command will be saved in java_cacerts.txt. Take a look at java_cacerts.txt. See if it includes the same certificate that is present in the browser by searching for a matching serial number. In the java_cacerts.txt file, the serial number will be in lowercase and without the “:” colon character. If it is not present, then this could be the reason for the error, and we can fix this by adding the certificate found in the browser. Back in the browser, export the Root CA. Choose the “X.509 Certificate (DER)” type, so the exported file has a der extension. Assuming the file is called example.der, pick the alias ‘example’ for this certificate. Next import the file. 1keytool -import -alias example -keystore /path/to/cacerts -file example.der You will be prompted for a password, use ‘changeit’ and respond “yes” on whether to trust this key. Dump the contents again to verify it contains your new certificate. Restart the JVM and check that it can now access the HTTPS URL. Also remove the java_cacerts.txt dump file. See also java-samples.com and keytool. 在 MAC 上可以通过 $(/usr/libexec/java_home)/jre/lib/security/cacerts 获取到 java cacerts 的路径，也可以在 Intellij 上看到； 导出证书可以通过 chrom 查看证书发布者（点击 url 左边安全按钮可以查看），然后在 mac 的 keychain 里面找到对应的证书，右键点击导出即可； 我用 MAC 导出的证书，然后使用上面 keytool 工具 import 后，依然出现上面所示问题，后面使用的 windows 下导出的证书，再用 keytool 工具 import 导入才成功；","categories":[],"tags":[{"name":"Security","slug":"Security","permalink":"http://yoursite.com/tags/Security/"}]},{"title":"Gre Config","slug":"Gre-Config","date":"2017-12-21T05:28:12.000Z","updated":"2017-12-21T05:35:57.928Z","comments":true,"path":"2017/12/21/Gre-Config/","link":"","permalink":"http://yoursite.com/2017/12/21/Gre-Config/","excerpt":"","text":"配置GRE隧道 **推荐 转自: http://blog.51cto.com/supercisco/293313 一、拓扑图： 二、配置及说明： 1、配置三台路由器的IP地址。并且在R1和R3配置默认路由，确保广域网链路能够通信： 1234567891011121314151617R1(config-line)#int s1/1R1(config-if)#no shR1(config-if)#ip add 202.101.172.37 255.255.255.252R1(config-if)#int lo0R1(config-if)#ip add 172.16.1.1 255.255.255.0R1(config-if)#ip add 172.16.2.1 255.255.255.0 seR1(config-if)#exitR1(config)#ip route 0.0.0.0 0.0.0.0 202.101.172.38 (配置一条默认路由) R3(config-line)#int s1/0R3(config-if)#no shR3(config-if)#ip add 218.108.248.202 255.255.255.252R3(config-if)#int lo0R3(config-if)#ip add 192.168.1.1 255.255.255.0R3(config-if)#ip add 192.168.2.1 255.255.255.0 seR3(config-if)#exitR3(config)#ip route 0.0.0.0 0.0.0.0 218.108.248.201 2、配置完之后，在R1 能够ping 通R3的WAN口： 123456R1(config)#do ping 218.108.248.202 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 218.108.248.202, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 12/27/64 ms 3、由于R2中没有到达私有网络的路由，在R1或R3都不能ping 通各自的回环接口IP： 1234567R1(config)#do ping 192.168.1.1 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.1.1, timeout is 2 seconds:U.U.USuccess rate is 0 percent (0/5)R1(config)# 4、满足条件之后，下面分别在R1和R3上通过GRE隧道技术把路由打通： 123456789R1(config)#interface tunnel 0 （启用GRE隧道）R1(config-if)#ip add 10.1.1.1 255.255.255.0 （为隧道配置IP地址）R1(config-if)#tunnel source serial1/1（配置隧道的本地源端口）R1(config-if)#tunnel destination 218.108.248.202 （配置隧道的目标出口，目的IP的可达性，是通过之前配置的本地默认路由保证的） R3(config)#int tunnel 0R3(config-if)#ip add 10.1.1.2 255.255.255.0R3(config-if)#tunnel source 218.108.248.202 （也可以指定隧道本地源IP）R3(config-if)#tunnel destination 202.101.172.37 5、R3配置完之后，马上就出来提示信息： 12R3(config-if)#*Mar 1 01:06:52.095: %LINEPROTO-5-UPDOWN: Line protocol on Interface Tunnel0, changed state to up 6、在R1上查看一下隧道接口信息： 1234567891011121314R1#sh interfaces tunnel0Tunnel0 is up, line protocol is up （状态为UP） Hardware is Tunnel （基于隧道的接口） Internet address is 10.1.1.1/24 MTU 1514 bytes, BW 9 Kbit, DLY 500000 usec, reliability 255/255, txload 1/255, rxload 1/255 Encapsulation TUNNEL, loopback not set （在原始的数据包上再封装了一个GRE报头） Keepalive not set Tunnel source 202.101.172.37 (Serial1/1), destination 218.108.248.202 Tunnel protocol/transport GRE/IP （隧道协议为GRE） Key disabled, sequencing disabled Checksumming of packets disabled Tunnel TTL 255 ……………… 7、查看一下R1的路由表： 1234567891011121314R1#sh ip route………… Gateway of last resort is 202.101.172.38 to network 0.0.0.0 202.101.172.0/30 is subnetted, 1 subnetsC 202.101.172.36 is directly connected, Serial1/1 172.16.0.0/24 is subnetted, 2 subnetsC 172.16.1.0 is directly connected, Loopback0C 172.16.2.0 is directly connected, Loopback0 10.0.0.0/24 is subnetted, 1 subnetsC 10.1.1.0 is directly connected, Tunnel0 （显示隧道接口为直连路由）S* 0.0.0.0/0 [1/0] via 202.101.172.38 （之前配置的默认路由） 8、在R1上能够ping 通隧道地址： 1234567R1#ping 10.1.1.2 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 10.1.1.2, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 12/34/68 msR1# 9、为了使R1和R3的loopback 地址能够通信，我们在R1和R3分别配置静态路由，下一跳指向隧道接口： 123R1(config)#ip route 192.168.0.0 255.255.0.0 tunnel 0 （下一跳可以指定隧道接口） R3(config)#ip route 172.16.0.0 255.255.0.0 10.1.1.1 （也可以指定隧道接口对端的IP） 注意： 这里设置的是 192.168.0.0 的路由走的是 GRE 端口，如果是访问其他网段，不会走 GRE 10、在R1或R3再次ping ，检查是否能ping 通对方私有网络回环接口IP地址： 12345678910111213R1(config)#do ping 192.168.1.1 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.1.1, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 8/38/108 msR1(config)#do ping 192.168.2.1 Type escape sequence to abort.Sending 5, 100-byte ICMP Echos to 192.168.2.1, timeout is 2 seconds:!!!!!Success rate is 100 percent (5/5), round-trip min/avg/max = 8/36/84 msR1(config)# 11、当然我们还可以查看一下tunnl 0的统计数据： 1234567R1#sh interface tunnel 0 statsTunnel0 Switching path Pkts In Chars In Pkts Out Chars Out Processor 30 3720 30 3720 Route cache 0 0 0 0 Total 30 3720 30 3720 注意：在配置隧道的时候一定要注意隧道的本地源端口和隧道的目的端口的路由可达性。","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"Mac Arp Route Table","slug":"Mac-Arp-Route-Table","date":"2017-12-21T05:01:51.000Z","updated":"2017-12-21T05:03:11.660Z","comments":true,"path":"2017/12/21/Mac-Arp-Route-Table/","link":"","permalink":"http://yoursite.com/2017/12/21/Mac-Arp-Route-Table/","excerpt":"","text":"转载自 : http://blog.51cto.com/dengqi/1223132 一：MAC地址表详解 说到MAC地址表，就不得不说一下交换机的工作原理了，因为交换机是根据MAC地址表转发数据帧的。在交换机中有一张记录着局域网主机MAC地址与交换机接口的对应关系的表，交换机就是根据这张表负责将数据帧传输到指定的主机上的。 交换机的工作原理 交换机在接收到数据帧以后，首先、会记录数据帧中的源MAC地址和对应的接口到MAC表中，接着、会检查自己的MAC表中是否有数据帧中目标MAC地址的信息，如果有则会根据MAC表中记录的对应接口将数据帧发送出去(也就是单播)，如果没有，则会将该数据帧从非接受接口发送出去(也就是广播)。 如下图：详细讲解交换机传输数据帧的过程 1)主机A会将一个源MAC地址为自己，目标MAC地址为主机B的数据帧发送给交换机。 2)交换机收到此数据帧后，首先将数据帧中的源MAC地址和对应的接口(接口为f 0/1) 记录到MAC地址表中。 3)然后交换机会检查自己的MAC地址表中是否有数据帧中的目标MAC地址的信息，如果有，则从MAC地址表中记录的接口发送出去，如果没有，则会将此数据帧从非接收接口的所有接口发送出去(也就是除了f 0/1接口)。 4)这时，局域网的所有主机都会收到此数据帧，但是只有主机B收到此数据帧时会响应这个广播，并回应一个数据帧，此数据帧中包括主机B的MAC地址。 5)当交换机收到主机B回应的数据帧后，也会记录数据帧中的源MAC地址(也就是主机B的MAC地址)，这时，再当主机A和主机B通信时，交换机根据MAC地址表中的记录，实现单播了。 如下图：当局域网存在多个交换机互联的时候，交换机的MAC地址表是怎么记录的呢？ 1)主机A将一个源MAC地址为自己，目标MAC地址主机C的数据帧发送给交换机 2)交换机1收到此数据帧后，会学习源MAC地址，并检查MAC地址表，发现没有目标MAC地址的记录，则会将数据帧广播出去，主机B和交换机2都会收到此数据帧。 3)交换机2收到此数据帧后也会将数据帧中的源MAC地址和对应的接口记录到MAC地址表中，并检查自己的MAC地址表，发现没有目标MAC地址的记录，则会广播此数据帧。 4)主机C收到数据帧后，会响应这个数据帧，并回复一个源MAC地址为自己的数据帧，这时交换机1和交换机1都会将主机C的MAC地址记录到自己的MAC地址表中，并且以单播的形式将此数据帧发送给主机A。 5)这时，主机A和主机C通信就是一单播的形式传输数据帧了，主机B和主机C通信如上述过程一样，因此交换机2的MAC地址表中记录着主机A和主机B的MAC地址都对应接口f 0/1。 总结：从上面的两幅图可以看出，交换机具有动态学习源MAC地址的功能，并且交换机的一个接口可以对应多个MAC地址，但是一个MAC地址只能对应一个接口。 注意：交换机动态学习的MAC地址默认只有300S的有效期，如果300S内记录的MAC地址没有通信，则会删除此记录。 二、ARP缓存表详解 上面我们讲解了交换机的工作原理，知道交换机是通过MAC地址通信的，但是我们是如何获得目标主机的MAC地址呢？这时我们就需要使用ARP协议了，在每台主机中都有一张ARP表，它记录着主机的IP地址和MAC地址的对应关系。 ARP协议：ARP协议是工作在网络层的协议，它负责将IP地址解析为MAC地址。 如下图：详细讲解ARP的工作原理。 1)如果主机A想发送数据给主机B，主机A首先会检查自己的ARP缓存表，查看是否有主机B的IP地址和MAC地址的对应关系，如果有，则会将主机B的MAC地址作为源MAC地址封装到数据帧中。如果没有，主机A则会发送一个ARP请求信息，请求的目标IP地址是主机B的IP地址，目标MAC地址是MAC地址的广播帧(即FF-FF-FF-FF-FF-FF)，源IP地址和MAC地址是主机A的IP地址和MAC地址。 2)当交换机接受到此数据帧之后，发现此数据帧是广播帧，因此，会将此数据帧从非接收的所有接口发送出去。 3）当主机B接受到此数据帧后，会校对IP地址是否是自己的，并将主机A的IP地址和MAC地址的对应关系记录到自己的ARP缓存表中，同时会发送一个ARP应答，其中包括自己的MAC地址。 4)主机A在收到这个回应的数据帧之后，在自己的ARP缓存表中记录主机B的IP地址和MAC地址的对应关系。而此时交换机已经学习到了主机A和主机B的MAC地址了。 路由表详解 路由器负责不同网络之间的通信，它是当今网络中的重要设备，可以说没有路由器就没有当今的互联网。在路由器中也有一张表，这张表叫路由表，记录着到不同网段的信息。路由表中的信息分为直连路由和非直连路由。 直连路由：是直接连接在路由器接口的网段，由路由器自动生成。 非直连路由：就是不是直接连接在路由器接口上的网段，此记录需要手动添加或者是使用动态路由。 路由表中记录的条目有的需要手动添加(称为静态路由)，有的测试动态获取的(称为动态路由)。直连路由属于静态路由。 路由器是工作在网络层的，在网络层可以识别逻辑地址。当路由器的某个接口收到一个包时，路由器会读取包中相应的目标的逻辑地址的网络部分，然后在路由表中进行查找。如果在路由表中找到目标地址的路由条目，则把包转发到路由器的相应接口，如果在路由表中没有找到目标地址的路由条目，那么，如果路由配置默认路由，就科举默认路由的配置转发到路由器的相应接口；如果没有配置默认路由，则将该包丢弃，并返回不可到达的信息。这就是数据路由的过程。 如下图：详细介绍路由器的工作原理 1)HostA在网络层将来自上层的报文封装成IP数据包，其中源IP地址为自己，目标IP地址是HostB，HostA会用本机配置的24位子网掩码与目标地址进行“与”运算，得出目标地址与本机不是同一网段，因此发送HostB的数据包需要经过网关路由A的转发。 2)HostA通过ARP请求获取网关路由A的E0口的MAC地址，并在链路层将路由器E0接口的MAC地址封装成目标MAC地址，源MAC地址是自己。 3)路由器A从E0可接收到数据帧，把数据链路层的封装去掉，并检查路由表中是否有目标IP地址网段(即192.168.2.2的网段)相匹配的的项，根据路由表中记录到192.168.2.0网段的数据请发送给下一跳地址10.1.1.2，因此数据在路由器A的E1口重新封装，此时，源MAC地址是路由器A的E1接口的MAC地址，封装的目标MAC地址则是路由器2的E1接口的MAC地址。 4)路由B从E1口接收到数据帧，同样会把数据链路层的封装去掉，对目标IP地址进行检测，并与路由表进行匹配，此时发现目标地址的网段正好是自己E0口的直连网段，路由器B通过ARP广播，获知HostB的MAC地址，此时数据包在路由器B的E0接口再次封装，源MAC地址是路由器B的E0接口的MAC地址，目标MAC地址是HostB的MAC地址。封装完成后直接从路由器的E0接口发送给HostB。 5)此时HostB才会收到来自HostA发送的数据。 总结：路由表负责记录一个网络到另一个网络的路径，因此路由器是根据路由表工作的。","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"Network Technologe","slug":"Network-Technologe","date":"2017-12-20T05:55:51.000Z","updated":"2017-12-20T06:05:19.687Z","comments":true,"path":"2017/12/20/Network-Technologe/","link":"","permalink":"http://yoursite.com/2017/12/20/Network-Technologe/","excerpt":"","text":"深入理解网络OSI 模型 网络四层模型 其中二层为数据链路层（网络接口层），三层为网络层（网间层），四层为传输层； Overlay 隧道模型","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"Network Problem","slug":"Network-Problem","date":"2017-12-16T05:43:27.000Z","updated":"2017-12-16T05:51:48.697Z","comments":true,"path":"2017/12/16/Network-Problem/","link":"","permalink":"http://yoursite.com/2017/12/16/Network-Problem/","excerpt":"","text":"简介本文档主要包含网络设置过程中碰到的问题； 具体问题在验证 pipework 过程中出现虚拟机网络不通的情况出现这种问题一遍需要查看如下方面的信息 查看 /etc/resolv.conf，看是否存在 nameserver 查看网卡是否存在 ip 地址 检查是否有路由及默认路由","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"ip 转发","slug":"ip-转发","date":"2017-12-15T05:24:49.000Z","updated":"2017-12-15T05:28:03.864Z","comments":true,"path":"2017/12/15/ip-转发/","link":"","permalink":"http://yoursite.com/2017/12/15/ip-转发/","excerpt":"","text":"简介​ IP转发的概念是，使 Linux 机器像路由器一样将数据从一个网络发送到另一个网络。所以，它能作为一个路由器或者代理服务器，实现将一个连接的互联网或者网络连接共享给多个客户端机器。 ip 转发使用的场景 在一台主机上，从一个网卡接收数据包，从另外一个网卡发送出去； 在一台主机上，从一个网卡接收数据包，发送到本机上的虚拟网桥，docker 需要宿主机开启 ip forward 的原因。 https://linux.cn/article-5595-1.html","categories":[],"tags":[]},{"title":"dpdk","slug":"dpdk","date":"2017-12-14T10:53:04.000Z","updated":"2017-12-14T10:58:21.331Z","comments":true,"path":"2017/12/14/dpdk/","link":"","permalink":"http://yoursite.com/2017/12/14/dpdk/","excerpt":"","text":"京东基于DPDK技术的高性能四层负载均衡器SKYLB2017-12-14 运维帮&amp;version=12020510&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100&amp;pass_ticket=sjbsLVBFYdJUQwhHgpMuH%2Fc7beO10%2BvpW6nGbiH97biBdUSXmChx98a6xvqGsrYv##) 作者：京东商城基础平台部 已授权运维帮订阅号发布 开源版本近期功能完善后会告知大家，敬请关注运维帮 摘要 随着京东业务的高速增长，作为应用入口的负载均衡，大流量大并发带来的挑战越来越严峻。本文主要介绍了京东商城设计和实践的一套高可靠，高性能的负载均衡器，我们命名为SKYLB。是一个使用intel DPDK报文转发库，实现运行在通用X86服务器上自研的分布式负载均衡服务。配合网络路由器的多重等价路由协议（OSPF）或者边际网关协议（BGP），组成承担京东数据中心核心四层负载均衡的集群。最大限度的发挥普通X86服务器硬件资源的性能，实现一套适合于京东商城业务的低成本，分布式，高性能，可扩展的智能负载均衡系统。 介绍 京东商城目前是国内最大的电商企业。京东的机房内部的流量爆炸式快速的增长。早在2016年初京东商城已经将所有的业务系统全部迁移到容器平台JDOS，线上百万+容器实例稳定运行。大流量的负载均衡的分配显得至关重要，也是京东商城新一代软件定义数据中心的关键基础服务设施之一。 负载均衡器一般介于网络上的路由器与后端服务器之间，负责将每个数据包通过一定的服务匹配，将其转发到后端服务器节点。充分考虑到京东商城数据中心全容器及全三层BGP组网的模型。以及基于DPDK的几乎达到网卡限速的性能，我们在设计负载均衡时，仅考虑实现了FULLNAT模式，即出向和入向的流量均通过负载均衡器，基本数据流程图如下图1所示： 图1 负载均衡流程图 一般根据业务及流量的规模的不同阶段来选择使用不同的负载均衡，通常我们在负载均衡的选择上大致有以下两个方向：1)硬件负载均衡，如F5。CitrixNetscaler等；2)软件负载均衡，如基于LVS，Haproxy，Nginx等开源软件来实现的负载均衡。对于上述两种负载均衡的选择，各有优缺点，如下： 1） 硬件负载均衡 可扩展性受限，无法跟上业务流量增长的需求。以及如618、双十一大促等瞬间流量高峰。 虽然可以成对部署避免单点故障，但是一般只能提供1+1冗余。 缺乏互联网快速迭代的灵活性，升级成本昂贵。 一般只能根据网络的情况来设定负载均衡，无关乎实际系统和硬件的情况。 成本较高。 2） 基于开源软件的负载均衡 可以根据实际系统和应用的状态来合理的负载，迭代、扩容和部署相对方便。 单台负载均衡性能相对较差，需要用集群来支撑负载均衡的整体性能。 性价比较低。 我们的目标： 设计实现一套高可靠、高性能、易维护及性价比高的L4负载均衡系统,。 基于通用X86_64服务器框架，以及支持DPDK网卡硬件，易开发和移植。 方便部署、删除和维护，集成到京东软件定义数据中心（JDOS2.0）系统，作为京东下一代软件定义数据中心的基础组件。 负载均衡下的服务器基于系统应用负载流量均摊，负载均衡器提供N+1 冗余，借助OSPF/BGP控制负载均衡器的流量负载。 基于系统应用级别的探活，自动故障检测及流量快速恢复。 本文主要介绍了SKYLB一种基于DPDK平台实现的快速可靠的软件网络负载均衡系统。不仅可以快速的横向扩展，还可以最大限度的提升负载均衡单个NIC的处理转发速度，来实现L4的负载均衡。借助DPDK的优势，如便利的多核编程框架、巨页内存管理、无锁队列、无中断poll-mode 网卡驱动、CPU亲和性等等来实现快速的网卡收发及处理报文，后续考虑TCP/IP 用户态协议实现和优化，进而实现L7负载均衡。 系统概览 工作场景 SKYLB部署在京东容器集群JDOS的前端，对于一个应用集群，发布一个或多个VIP到SKYLB服务上，当客户端需要访问应用节资源URL，首先通过域名访问JD智能分布式DNS服务（SKYDNS详见https://github.com/ipdcode/skydns）, SkyDns会智能返回当前最近且状态正常且负载正常的VIP服务的IP,客户端就会向VIP去请求连接。 SKYLB节点上运行了一个路由发布服务agent，我们使用该agent与开启OSPF/BGP的路由器做路由交互，当SKYLB的上层路由器接收到请求VIP的数据包时，路由器通过（OSPF/BGP）的等价多路径转发协议选择一个可以使用的发布该VIP的SKYLB节点，将报文转发给一个SKYLB节点。通过这种策略来实现VIP的发布和横向容量负载能力扩展。 当报文通过上述步骤到达SKYLB负载均衡后，通过常用的负载均衡策略(round robin，一致性hash ，最小连接数)，将数据包送到相应的后端容器服务。 系统架构 JingdongDatacenter Operating System(JDOS) 是基于JDOS提供物理机/虚拟机/容器的统一管理系统、配备灵活的网络互连、可靠的分布式共享存储，实现软件定义数据中心的计算资源统一管理和集群管理。通过使用JDOS，可以直接迅速得到任意需要的计算、存储、网络、安全等方面的资源和能力。SKYLB作为整个JDOS系统的一个重要组成部分，负责提供集群的L4负载均衡能力，通过restful API等接口与JDOS系统交互。用户可以通过统一调度管理平台便捷的创建、删除、迁移负载均衡系统。同时多个应用服务进行流量分发的负载均衡服务，从而扩展应用系统对外的服务能力，并且通过消除单点故障提高应用系统的可用性。 系统的基本架构如下图2所示，每一个集群配备一组可容灾的负载均衡控制中心,主要通过restful api接口负责与JDOS调度中心交互，接收vip的配置请求。同时我们在每一个SKYLB的节点上运行一个代理进程，该代理进程通过gRPC与控制中心连接。接收控制中心下达的创建及删除vip,后端server endpoint服务等一系列指令，通过load balancer 提供的命令行执行相应的指令。接收load balancer 关于流量及报文的监控信息，对于流量及监控进行告警，并且通知控制中心和调度中心进行扩容等操作。 代理进程同时还负责后端服务 server endpoint基于服务可用性的健康检查，及时根据后端服务的状态通过命令行进行添加和删除的操作。 图2 系统架构图 优势 1）扩展性 支持动态添加和删除后端服务的容器，实现无缝的伸缩；在伸，缩过程中，对相关调用和访问者无影响。 2）高可用性 提供多活负载均衡，有多个VIP，它们对应一个域名，自研DNS服务SKYDNS会根据请求的客户端IP智能解析可用的VIP，返回给用户，从而实现更高的可用性；即使一个VIP不可用，也不会影响业务系统对外提供服务。同时借助OSPF/BGP等协议实现负载均衡的横向扩充 3）服务能力自动可调 SKYLB根据VIP实际接收流量的负载需要调整负载均衡的服务能力，比如流量、连接数的控制等指标。 功能特点 1）协议支持 负载均衡支持包含TCP、UDP协议的四层负载均衡，配备健全的链路跟踪机制，以及多种调度策略，用户可以根据服务的需要创建合适自己的负载均衡。 2）高可用性 支持容器的健康检查，除传统的IP+Port，并加入对URL检查，保证应用可用性： 健康检查频率可自定义；一旦探测到异常，则不会将流量再分配到这些异常实例，保证应用可用性。 3）集群部署，多层次容错机制： 负载均衡采用集群部署，支持热升级，机器故障和集群维护对用户完全透明，结合DNS使用还可支持全局负载均衡。 4）灵活性 支持多种流量调度算法，使得流量分配更均匀： 负载均衡支持加权轮询和最小连接数这两种调度算法，可根据自身需求选择相应的算法来分配用户访问流量，并支持设置后端容器权重，使得流量调度更均匀，提升负载均衡能力。 支持会话保持，满足用户个性化需求： 负载均衡通过IP地址实现会话保持，可将一定时间内来自同一用户的访问请求，转发到同一个后端容器上进行处理，从而实现用户访问的连续性。 5）易用性 提供多种管理途径，轻松操纵负载均衡： 用户可通过控制台轻松实现负载均衡器的配置、释放等功能。后续会开放标准的API或SDK提供给用户，从而自己开发对负载均衡的控制管理。 系统设计及技术实现 负载均衡模式选择 常用的负载均衡模式有DR，NAT，TUNNEL，FULLNAT。每种模式都有自己的优势和使用场景，业内对每种模式的分析比较文档很多，不再介绍。由于JDOS容器网络需要VLAN隔离，而FULLNAT刚好支持LB和RS跨VLAN通信，结合我们自身容器集群的需求，我们在实现SKYLB时主要考虑支持FULLNAT模式。图3是SKYLB的FULLNAT负载均衡模式中数据包的流向图。SKYLB位于客户端和后端服务之间，对于客户端的请求报文，将目的地址替换成后端服务的地址，源地址替换成SKYLB的本地地址，对于后端服务的响应报文，将目的地址替换成客户端地址，源地址替换成SKYLB的VIP地址。 图3 FULLNAT模式下SKYLB的数据包流向图 借助DPDK实现高速转发 Data Plane DevelopmentKit（DPDK）：是运行在Linux 用户态，实现X86通用平台网络报文快速处理的库和驱动的集合，如下图4所示，其主要特点： • 多核编程框架及CPU亲和性 每个NUMA节点有单独的CPU和本地内存 CPU访问本地内存速度比访问远端内存快，避免CPU访问远端内存 注意网卡挂载的NUMA节点巨页内存管理 • 巨页(HugePage) 普通内存页面大小4KB，巨页内存页面大小2MB/1GB 减少页表项数目，降低TLB miss 使用大页面比使用4K的页面性能提高10%~15% 零拷贝，报文数据及转发内存零拷贝。 • 无锁队列 使用无锁队列，入队出队无需阻塞等待锁资源 • poll-mode网卡驱动 DPDK网卡驱动完全抛弃中断模式，基于轮询方式收包 图4 DPDK相关模块 包处理架构实现 图5是SKYLB基于RTC数据包处理模型实现的架构。SKYLB选择一个核作为控制核，执行命令配置，与内核交换，ARP表维护等任务。其他的核作为工作核，每个工作核轮询网卡的一个RX队列，执行数据包处理任务。SKYLB利用网卡的RSS功能实现客户端请求报文的分流，利用网卡FDIR功能实现后端服务响应报文的分流。SKYLB对报文分流目的是要保证客户端的请求报文和其对应的服务端响应报文到达同一个工作核上。在这个目的达成的前提下，SKYLB的业务实现会简单和高效很多。每个工作核维护一张session表，同于保存客户端和后端服务的连接信息。SKYLB不需要考虑对session表加锁，因为每个工作核的session表都是独立的。 图5 SKYLB的RTC包处理模型框架图 我们设计SKYLB每个工作核独占至少一个LIP，并将LIP信息写入网卡配置。图6是网卡对IP报文分流的流程图。图中dst_ip即SKYLB为每个工作核分配的LIP。网卡对后端服务响应报文的目的地址LIP匹配成功，将报文送到绑定的RX队列，没有匹配的报文则可以认为是客户端的请求报文，按RSS流程分配RX队列。 图6 网卡对IP报文分流过程图 SKYLB在启动过程中还会为每个物理口创建一个KNI接口，控制核负责轮询KNI接口。该接口主要用于外部程序quagga向路由器发布VIP信息，Agent检查后端服务健康状态。 SKYLB目前支持的负载均衡调度算法有一致性hash，round robin和最小连接数算法。 Session五元组，SKYLB采用五元组来实现会话的管理功能，如下图7 所示： 图7 SKYLB 五元组管理Session 负载均衡冗余设计 SKYLB使用BGP或者OSPF的模式组成集群，通过上述协议将数据包散列到集群中各个节点上，保证单台SKYLB故障或者恢复后能动态的将机器添加及删除。其冗余实现设计如下图6所示： 图8 SKYLB的冗余设计 性能优化实践 良好的流程设计是性能提升的关键，这方面的流程涉及和业务相关，缺乏共性，因此不做详细阐述。主要介绍SKYLB性能优化过程中使用的其他优化方法。 恰当地使用rte_prefetch0()，可以减少cache-miss次数，避免当前函数成为性能热点。性能调优工具perf可以帮助我们分析应该在哪处代码使用预取。例如我们使用perf发现报文处理函数中有一个处代码是性能热点，该代码用于读取新报文的类型字段并判断，分析认为很可能是cache-misses造成的。在进入报文处理函数前使用rte_prefetch0()，有效避免该函数成为热点。 恰当地使用likely()和unlikely()，可以减少分支预测失败的次数。我们在SKYLB代码的一处分支语句中使用unlikely()优化，性能提升明显。分支预测优化点可以借助perf分析确定，也可以根据自己对代码执行流程的理解确定。 尽量减少锁的使用。SKYLB中配置信息不经常变化，我们没有单独为每个可能争用的资源加锁，而是只用一把DPDK提供的读写锁，每个读线程在加读锁后，处理一批报文，然后释放读锁。既简化流程，又减少了操作读写锁的开销(DPDK读写锁的开销并不是很大)。 性能数据 测试环境： CPU：Intel(R) Xeon(R) CPU E5-2640 v3 NIC：intel 82599ES 10-GigabitSFI/SFP+ Network Connection 测试配置： 负载均衡模式：FULLNAT 调度算法：一致性hash 配置：CPU占用8核 内存占用4G 性能测试数据： 1）UDP发包，测试转发性能，我们使用了SKYDNS作为后端服务，客户端采用UDP请求DNS 线程数 Avg**（ms）时延** TPS**（笔/秒）** 错误数 错误数 3000 1 4343070 0 0.00% 表1 SKYLB基于UDP的DNS性能测试数据 ​ 2）NGINX作为后端服务，压测HTTP性能。 ​ 配置：CPU占用8核 内存占用4G 线程数 Avg**（ms）时延** TP99**（ms）** TPS**（笔/秒）** 错误数 错误数 1100 0 2 2101170 0 0.00% 表2 SKYLB HTTP性能测试数据 图9 SKYLB HTTP 性能测试指标图 图9 SKYLBhttp 性能测试指标图 总结 本文主要介绍了SKYLB，一种的基于intel DPDK平台开发的负载均衡器。其接近网卡线速处理及转发能力，灵活的部署，多样的监控以及可靠的稳定性。基本上覆盖所有4层负载均衡的业务处理需求，配合集群管理以及调度，作为京东数据中心操作系统（JDOS）的一个重要的组成部分，在京东数据中心发挥至关重要的作用。 参考资料 Google Maglev：A Fast and Reliable SoftwareNetwork Load Balancer。 DPDK：http://dpdk.org/doc/guides/prog_guide/","categories":[],"tags":[]},{"title":"docker command operation","slug":"docker-command-operation","date":"2017-12-14T09:00:06.000Z","updated":"2017-12-14T09:00:06.477Z","comments":true,"path":"2017/12/14/docker-command-operation/","link":"","permalink":"http://yoursite.com/2017/12/14/docker-command-operation/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"docker 之创建网桥","slug":"docker-之创建网桥","date":"2017-12-14T05:05:26.000Z","updated":"2017-12-14T05:17:19.044Z","comments":true,"path":"2017/12/14/docker-之创建网桥/","link":"","permalink":"http://yoursite.com/2017/12/14/docker-之创建网桥/","excerpt":"","text":"docker daemon 创建网桥docker daemon启动时会通过参数创建网桥或者使用已经存在的网桥。 源码12345678910111213141516171819202122232425// Create the actual bridge device. This is more backward-compatible than// netlink.NetworkLinkAdd and works on RHEL 6.func CreateBridge(name string, setMacAddr bool) error &#123; if len(name) &gt;= IFNAMSIZ &#123; return fmt.Errorf(&quot;Interface name %s too long&quot;, name) &#125; s, err := getIfSocket() if err != nil &#123; return err &#125; defer syscall.Close(s) nameBytePtr, err := syscall.BytePtrFromString(name) if err != nil &#123; return err &#125; if _, _, err := syscall.Syscall(syscall.SYS_IOCTL, uintptr(s), SIOC_BRADDBR, uintptr(unsafe.Pointer(nameBytePtr))); err != 0 &#123; return err &#125; if setMacAddr &#123; return setBridgeMacAddress(s, name) &#125; return nil&#125; 从实现代码可以看出，docker 是使用系统调用 syscall 创建网桥；","categories":[],"tags":[]},{"title":"vpc introduce","slug":"vpc-introduce","date":"2017-12-12T09:46:44.000Z","updated":"2017-12-12T09:47:12.643Z","comments":true,"path":"2017/12/12/vpc-introduce/","link":"","permalink":"http://yoursite.com/2017/12/12/vpc-introduce/","excerpt":"","text":"VPC 兴起的背景随着云计算的不断发展，对虚拟化网络的要求越来越高，比如网络的弹性（scalability）、安全性（security）、可靠性（reliability）以及私密性（privacy）等，并且还要求具有极高的互联性能（performance），因此催生了多种多样的网络虚拟化技术。 比较早的解决方案，是将虚拟机的网络和物理网络融合在一起，形成一个扁平的网络架构，例如大二层网络。随着虚拟化网络规模的扩大，这种方案中的 ARP 欺骗、广播风暴、主机扫描等问题会越来越严重。为了解决这些问题，出现了各种网络隔离技术，把物理网络和虚拟网络彻底隔开。其中一种技术是用户之间用 VLAN 进行隔离，但是 VLAN 的数量最大只能支持到 4096 个，无法支撑公共云的巨大用户量。 VPC 原理描述基于目前主流的隧道技术，专有网络（Virtual Private Cloud，简称VPC）隔离了虚拟网络。每个 VPC 都有一个独立的隧道号，一个隧道号对应着一张虚拟化网络。一个 VPC 内的 ECS 之间的传输数据包都会加上隧道封装，带有唯一的隧道 ID 标识，然后送到物理网络上进行传输。不同 VPC 内的 ECS 因为所在的隧道 ID 不同，本身处于两个不同的路由平面，所以不同 VPC 内的 ECS 无法进行通信，天然地进行了隔离。 VPC 逻辑框架VPC 包含交换机、网关和控制器三个重要的组件。 交换机和网关组成了数据通路的关键路径，控制器使用自研的协议下发转发表到网关和交换机，完成了配置通路的关键路径。整体架构里面，配置通路和数据通路互相分离。 交换机是分布式的结点，网关和控制器都是集群部署并且是多机房互备的，并且所有链路上都有冗余容灾，提升了 VPC 产品的整体可用性。 在产品上，除了给您一张独立的虚拟化网络，腾讯云还为每个 VPC 提供了独立的路由器、交换机组件，让您可以更加丰富地进行组网。 针对内网安全需求，您可以使用安全组功能在一个 VPC 内进行更加细粒度的访问控制和隔离。缺省情况下，VPC 内的 ECS 只能和本 VPC 内其它 ECS 或其它云服务之间进行通信。此外，您可以使用阿里云提供的和 VPC 相关的产品，比如弹性公网 IP 和高速通道，使 VPC 可以和 Internet、其它 VPC、本地自有的网络（如用户办公网络、用户数据中心）之间进行通信。","categories":[],"tags":[]},{"title":"kafka design","slug":"kafka-design","date":"2017-12-11T06:45:40.000Z","updated":"2017-12-11T08:42:14.279Z","comments":true,"path":"2017/12/11/kafka-design/","link":"","permalink":"http://yoursite.com/2017/12/11/kafka-design/","excerpt":"","text":"转译：Varnish 架构师笔记 找到这篇文章是在阅读 Kafka 文档时，一个名为 “Don’t fear the filesystem!”的段落中提到的。文档指出，我们总是思维定势地以为磁盘很慢，内存很快。然而今天的计算机体系结构中，并非这么简单： 因为操作系统 PageCache 的存在，磁盘操作可能很快 虽然磁盘 IOPS 难以提高，但吞吐量在不断上升；换句话说，顺序读写磁盘非常快 CPU Cache 常常被忽略了，了解 CPU Cache 对提升内存读写性能至关重要 原文链接 当你开始深入 Varnish 的源代码后，应该会发觉它与你日常所见的一般应用软件有着明显不同，而这绝非偶然。 多年以来我的绝大部分时间花费在 FreeBSD 的内核开发上，而每每涉足用户空间编程，却总是毫无例外地发现那里的人们还在以1975年的方式工作。 所以 Varnish 这个项目一开始并没能激起我很大的兴趣。但我渐渐意识到 Varnish 虽然是一个用户态应用，但却也是一个能充分发挥我长久以来积累的关于硬件和内核的经验知识的理想场所。目前 Varnish 的开发已经进展到alpha版本的阶段，而我觉得应当承认自己相当享受这一段历程。 1975的编程方式到底出了什么问题？简而言之：计算机系统再也不应该被看作是由两个存储层次构成的了。 首先是主存：从水银延迟线，磁芯存储器，到现在可供随机访问的RAM。 然后是辅存：从纸带，磁带，到磁盘。最早的磁盘有一屋子大，然后缩小到洗衣机的尺寸，到今天硬盘可以被放进随身携带的 MP3 播放器中。 于是大家就按照这样的划分，在内存中分配变量，在磁盘中存取数据，以这样的方式进行编程工作。 还是拿 Squid 这个1975年风格的系统为例：你事先配置它的内存和硬盘用量，然后它会花费大把时间来追踪哪些HTTP对象驻留内存中，哪些存放在硬盘上，并且根据不同情况来调整它们的放置策略。 然而实际上呢，现今的计算机应被视为只使用一种统一的存储系统，这个系统完全基于硬盘（磁盘，固态硬盘或者其他什么东西），而传统的内存呢，在操作系统内核和虚拟内存硬件机制的帮助下可以被看作是硬盘的缓存。 回过头来看 Squid 的策略，它精心设计的存储管理机制实际上却陷入了与操作系统内核同样精巧的管理策略的激烈冲突。而就像所有内战一样，这样的冲突必然一事无成。 我们可以尝试从细节角度来看整个流程：一开始 Squid 会请求内存用来创建了一个 HTTP 对象，它往往会在创建之初被频繁访问多次，然后闲置一段时间。而后当内核接收到其他内存分配请求时，会将这些它认为闲置的内存数据放到硬盘交换分区去，而把这些回收的内存页用于更活跃的任务。在Squid下一次访问这一对象时，操作系统又会把暂存在交换分区的数据取回来供它使用。 这些内核对于内存的操作对于 Squid 是透明的，在它看来这个 HTTP 对象就像从没离开过内存一样，而实际上物理内存页得到了更有效的使用。 这就是虚拟内存机制。 如果事情到此为止的话就好了，但接下来1975年式的编程风格就出现了。 一段时间之后，Squid 也和内核一样注意到了这个对象闲置了，于是打算把它放到硬盘上去，省出一些内存来给更频繁访问的数据使用。所以它打开一个文件把这个对象写了进去。 打开慢镜头来看这个写入的过程： Squid 通过系统调用 write 将 HTTP 对象的虚拟内存地址传递给内核。由于物理页已经被内核交换出去，这个内存访问将引发一个缺页中断。为了重新载入这个内存页，内核不得不交换出另一个正在使用的内存页（很可能包含另一 Squid 的对象），修复页表，然后继续执行系统调用。Squid 对这些一无所知，它自以为只是进行了一次再普通不过的访存操作而已。 接下来 Squid 可以终于拿这块内存放别的数据了。而当这个对象再次被访问到时，Squid 又不得不把它从硬盘中取回来。首先它需要空闲的内存来存放这个对象，于是它根据某种淘汰算法选中另一个最近不常用的对象，把它写到硬盘上去（上面那些步骤重演了一遍）。然后打开文件读出这次所需的那个对象，最后通过网络套接字发送出去。 这一切显然充满了各种无用功。 让我们看看 Varnish 是怎么做的 Varnish 会直接请求大块虚拟内存，并由操作系统将这个内存空间映射到一个硬盘文件。当需要访问某个 HTTP 对象时，只需要正确索引这个对象相应的虚拟内存地址，剩下的交给操作系统就好了。当内核需要回收一些内存页时，它会自行决定将一些 Varnish 的内存数据写回到映射的文件中。而当 Varnish 再次访问这一块虚拟内存时，内核自然会腾出内存页来将文件中的数据读回使用。 仅此而已。 Varnish 不去尝试控制哪些数据应该被缓存在内存中，哪些应该放到硬盘上去。内核代码和硬件会处理这些事情，而且处理得很漂亮。 此外，与 Squid 不同的是 Varnish 只需要一个文件而不是为每个 HTTP 对象创建单独的文件。没有任何理由需要在 HTTP 对象和文件系统对象间建立一一对应的关系，也没有理由把时间浪费在文件系统的命名空间处理上。Varnish 需要处理的只是虚拟内存的指针和所需对象的长度值而已。 虚拟内存的出现为数据大于物理内存的场景提供了一种便利的机制，但人们似乎并没有领悟这一点。 更多的缓存CPU 的时钟频率目前看来基本止步于4GHz了。即便如此，为了避免内存读写的瓶颈，硬件工程师们不得不使用使用一级，二级，有时候甚至是三级 CPU cache（现在我们可以认为 RAM 是第四级缓存了），此外还有写缓冲，流水线，页模式读取等各种技术，而这些都是为了加快访存来匹配CPU的处理速度。 虽然时钟频率方面受限，但硅工艺的进步缩小了器件尺寸，集成了更多的晶体管。所以多核 CPU 的设计逐渐成为主流，但这从编程模型角度看来实在是很糟糕的一件事。 虽然多核系统存在已久，但编写能够利用上多个 CPU 的代码依然是一件棘手的事。而要在多核系统上写出高性能的程序就更是如此了。 比如我们有两个计数器： 12unsigned n_foo;unsigned n_bar; 在一个 CPU 上执行了 n_foo++ 的操作会使得CPU读取 n_foo 的值然后写回。 读取一个内存位置首先会检查它是否在 CPU L1 cache 中命中，这挺难的除非它刚刚被使用过。接下来是 L2 cache，我们不妨假设依然没有命中吧。 如果是在一个单核系统上，CPU 会去内存读取数据然后就完事。但在多核系统中，我们必须去检查其他CPU核心是否缓存并修改了 n_foo 的数值。我们会发起一个特殊的总线事务做这种检查，如果其他 CPU 答复说它确实持有这样一份拷贝，它就需要将它写回到内存中。如果硬件设计良好，我们可能可以通过总线监听获得这份新数据，不然的话就需要去内存里读取它。 我们终于可以修改 n_foo 的值了，但其实修改完了之后一般不会直接将它写回到内存中。为了之后操作的快速存取，很可能我们会把它缓存起来。 现在假设另一个 CPU 需要执行 n_bar++ 的操作，它能够直接进行吗？答案是否定的。因为缓存的单位并不是字节而是 cache-line（典型值是 8-128 个字节）。所以当第一个 CPU 忙着处理 n_foo 时，第二个 CPU 必须等待，因为虽然它想获取的是另一个变量，但却不幸落在了同一个 cache-line 中。 明白了吧？没错，这有点丑。 我们该怎么办尽一切可能，减少内存操作。 下面是 Varnish 的一些做法。 当需要处理一个 HTTP 请求或响应时，我们会持有一组指针和一个内存工作区。我们并不需要在处理每个 HTTP 报头时都调用 malloc，而是一次性分配整个工作区的内存，然后按需从中获取所需空间。而当我们一次性释放全部报头时，只要将指针重置到工作区的起始位置即可。 当需要将 HTTP 报头从一个请求拷贝到另一个请求（或从从一个响应复制到另一个响应）时，并不需要进行字符串拷贝，而只要拷贝指针。如果源报头在这个过程中不会被不释放或改写，这样做是非常安全的。比如从客户端请求到后台请求的拷贝就是这样一个例子。 但在一些新构建的报头生命周期长于源报头的场景中，就需要另外分配内存了。例如当我们会缓存新 HTTP 对象时，首先就计算整个报头所需空间，然后通过一次 malloc 调用来获取内存。 另外我们会尽可能重用那些正被缓存的内存数据。 比如 Varnish 的 worker 线程是以最近最忙的方式调度的，也即是说一个 worker 线程空闲后会被放回到队列的最前端，使得它更有机会马上去处理下一个新请求，这样它的数据，栈空间和变量等很可能可以在 CPU 缓存中被重用，而不是再次从RAM中读取。 同时对于 worker 线程经常使用的数据，我们会把它们分配在每个线程的栈变量中，并且确保它们占据完整的内存页。这样我们就可以尽可能避免 cache-line 的竞争。 如果对你来说这些听起来都很陌生，我可以告诉你它们是确实有效的：Varnish 处理一个命中缓存的请求最多只需18个系统调用，而且其中不少只是为了获得时间戳来满足统计的需要。 这些技术并不新鲜，我们已经在内核开发中使用了10多年，现在该轮到你们来学习了:-) 如此，欢迎进入 Varnish，一个 2006风格架构的程序。 Linux cache reference : https://www.ibm.com/developerworks/cn/linux/l-cache/index.html","categories":[],"tags":[]},{"title":"kafka","slug":"kafka","date":"2017-12-11T03:16:36.000Z","updated":"2017-12-11T03:31:34.102Z","comments":true,"path":"2017/12/11/kafka/","link":"","permalink":"http://yoursite.com/2017/12/11/kafka/","excerpt":"","text":"kafka internalkafka operator list kafka brokers 123456789tianyuan# ./bin/zookeeper-shell.sh localhost:2181/kafpush &lt;&lt;&lt; &quot;ls /brokers/ids&quot;Connecting to localhost:2181/kafpushWelcome to ZooKeeper!JLine support is disabledWATCHER::WatchedEvent state:SyncConnected type:None path:null[3, 2, 1] 注意： ./bin/zookeeper-shell.sh localhost:2181 会连接到kafka所在的zk集群 create topic 1./bin/kafka-topics.sh --create --zookeeper localhost:2181/kafpush --replication-factor 1 --partitions 300 --topic test alter topic partitions 1./bin/kafka-topics.sh --alter --zookeeper localhost:2181/kafpush --partitions 300 --topic test","categories":[],"tags":[]},{"title":"pipework demo","slug":"pipework-demo","date":"2017-12-07T12:58:07.000Z","updated":"2017-12-16T05:45:26.135Z","comments":true,"path":"2017/12/07/pipework-demo/","link":"","permalink":"http://yoursite.com/2017/12/07/pipework-demo/","excerpt":"","text":"pipework demo主机A地址为10.10.101.105/24,网关为10.10.101.254,需要给Docker容器的地址配置为10.10.101.150/24。在主机A上做如下操作： 12345678910111213141516#安装pipeworkgit clone https://github.com/jpetazzo/pipeworkcp ~/pipework/pipework /usr/local/bin/#启动Docker容器。docker run -itd --name test1 ubuntu /bin/bash#配置容器网络，并连到网桥br0上。网关在IP地址后面加@指定。#若主机环境中存在dhcp服务器，也可以通过dhcp的方式获取IP#pipework br0 test1 dhcppipework br0 test1 10.10.101.150/24@10.10.101.254#将主机eth0桥接到br0上，并把eth0的IP配置在br0上。这里由于是远程操作，中间网络会断掉，所以放在一条命令中执行。#将主机eth0桥接到br0上，eth0就成了虚拟网桥br0的一部分。ip addr add 10.10.101.105/24 dev br0; \\ ip addr del 10.10.101.105/24 dev eth0; \\ brctl addif br0 eth0; \\ ip route del default; \\ ip route add default gw 10.10.101.254 dev br0 12345678910111213141516171819202122232425#创建br0网桥#若ovs开头，则创建OVS网桥 ovs-vsctl add-br ovs*#IFNAME=br0brctl addbr $IFNAME#创建veth pair,用于连接容器和br0#LOCAL_IFNAME=eh0 GUEST_IFNAME=eth1ip link add name $LOCAL_IFNAME mtu $MTU type veth peer name $GUEST_IFNAME mtu $MTU#找到Docker容器test1在主机上的PID,创建容器网络命名空间的软连接#GUESTNAME=test1DOCKERPID=$(docker inspect --format=&apos;&#123;&#123; .State.Pid &#125;&#125;&apos; $GUESTNAME)ln -s /proc/$NSPID/ns/net /var/run/netns/$NSPID#将veth pair一端放入Docker容器中，并设置正确的名字eth1#CONTAINER_IFNAME=eth1#把$GUEST_IFNAME添加到$NSPID网络命名空间中，也就是虚拟网络环境ip link set $GUEST_IFNAME netns $NSPIDip netns exec $NSPID ip link set $GUEST_IFNAME name $CONTAINER_IFNAME#将veth pair另一端加入网桥#若为OVS网桥则为 ovs-vsctl add-port $IFNAME $LOCAL_IFNAME $&#123;VLAN:+&quot;tag=$VLAN&quot;&#125;brctl addif $IFNAME $LOCAL_IFNAME#为新增加的容器配置IP和路由#GATEWAY=10.10.101.254ip netns exec $NSPID ip addr add $IPADDR dev $CONTAINER_IFNAMEip netns exec $NSPID ip link set $CONTAINER_IFNAME upip netns exec $NSPID ip route delete defaultip netns exec $NSPID ip route add $GATEWAY/32 dev $CONTAINER_IFNAME 首先pipework检查是否存在br0网桥，若不存在，就自己创建。若以”ovs”开头，就会创建OpenVswitch网桥，以”br”开头，创建Linux bridge。 创建veth pair设备，用于为容器提供网卡并连接到br0网桥。 使用docker inspect找到容器在主机中的PID，然后通过PID将容器的网络命名空间链接到/var/run/netns/目录下。这么做的目的是，方便在主机上使用ip netns命令配置容器的网络。因为，在Docker容器中，我们没有权限配置网络环境。 将之前创建的veth pair设备分别加入容器和网桥中。在容器中的名称默认为eth1，可以通过pipework的-i参数修改该名称。 然后就是配置新网卡的IP。若在IP地址的后面加上网关地址，那么pipework会重新配置默认路由。这样容器通往外网的流量会经由新配置的eth1出去，而不是通过eth0和docker0。(若想完全抛弃自带的网络设置，在启动容器的时候可以指定–net=none) ip netns相关命令 增加虚拟网络命名空间 1ip netns add net0 显示所有的虚拟网络命名空间 123EULER:~ # ip netns listnet0 也可通过查看/var/run/netns目录下的文件来list 123EULER:~ # ls /var/run/netns/net0 进入虚拟机网络环境 12345678910111213ip netns exec net0 `command` 如EULER:~ # ip netns exec net0 bash #打开虚拟网络环境net0的bash窗口EULER:~ # ip addr #显示所有虚拟网络环境的设备EULER:~ # exit #退出该网络虚拟环境exit 增加一对veth虚拟网卡 1EULER:~ # ip link add type veth 将veth0添加到net0虚拟网络环境 1ip link set veth0 netns net0 将虚拟网卡veth1改名并添加到net1虚拟网络环境中 1ip link set dev veth1 name net1-bridge netns net1 设置虚拟网络环境net0的veth0设备处于激活状态 1ip netns exec net0 ip link set veth0 up 为虚拟网络环境net0的veth0设备增加IP地址 1ip netns exec net0 ip address add 10.0.1.1/24 dev veth0 在操作过程中遇到的问题 添加路由时的错误 12ip route add default gw 10.211.55.1 dev eth0Error: either &quot;to&quot; is duplicate, or &quot;gw&quot; is a garbage. 需要修改成： ip route add 10.221.55.0/24 via 10.211.55.1 dev eth0","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"ansible 通过跳板机管理远程服务器","slug":"ansible-通过跳板机管理远程服务器","date":"2017-12-07T01:52:19.000Z","updated":"2017-12-07T02:09:40.629Z","comments":true,"path":"2017/12/07/ansible-通过跳板机管理远程服务器/","link":"","permalink":"http://yoursite.com/2017/12/07/ansible-通过跳板机管理远程服务器/","excerpt":"","text":"环境信息 mac笔记本 跳板机：jump.example.com 被管理节点：node1.example.com, node2.example.com, node3.example.com mac 笔记本无法直接连接被管理节点，需要通过跳板机进行管理； 修改ssh config123456Host node1.example.com node2.example.com node3.example.com ForwardAgent yes User user Port 22 IdentityFile ~/.ssh/net_id_rsa ProxyCommand ssh -qaY -i ~/.ssh/net_id_rsa -p 1046 user@jump.example.com &apos;nc -w 14400 %h %p&apos; 注意：我这边从跳板机到被管理节点和mac到跳板机的私钥是一样的，都是 net_id_rsa 如果不一样，需要修改 IdentityFile 和 ProxyCommand 的 -i 参数 IdentityFile是跳板机登陆被管理节点使用的私钥文件，ProxyCommand -i 参数使用的私钥是mac登陆跳板机所使用的私钥 修改ansible inventory12345host file:[node]node1 ansible_host=node1.example.comnode2 ansible_host=node2.example.comnode3 ansible_host=node3.example.com 验证1ansible -i host -m ping","categories":[],"tags":[]},{"title":"consul deploy","slug":"consul-deploy","date":"2017-12-05T08:21:16.000Z","updated":"2017-12-05T08:37:42.461Z","comments":true,"path":"2017/12/05/consul-deploy/","link":"","permalink":"http://yoursite.com/2017/12/05/consul-deploy/","excerpt":"","text":"consul 集群部署从consul github clone 工程~/src/github.com/hashicorp/consul 修改 Vagrantfile 文件~/src/github.com/hashicorp/consul/demo/vagrant-cluster/Vagrantfile 在最后增加: 1234config.vm.define &quot;n3&quot; do |n3| n3.vm.hostname = &quot;n3&quot; n3.vm.network &quot;private_network&quot;, ip: &quot;172.20.20.12&quot;end 启动虚拟机使用vagrant up启动虚拟机 部署信息 HostName IP Address Consul Role n1 172.20.20.10 bootstrap consul server n2 172.20.20.11 server n3 172.20.20.12 client n1 config: /etc/consul.d/bootstrap/config.json12345678910111213&#123; \"bootstrap\": true, \"server\": true, \"node_name\": \"ageng_one\", \"bind_addr\": \"172.20.20.10\", \"client_addr\": \"172.20.20.10\", \"datacenter\": \"dc1\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"6MmrecCBuJMePdP2tOBDxw==\", \"log_level\": \"INFO\", \"enable_script_checks\": true, \"enable_syslog\": true&#125; n2 config: /etc/consul.d/server/config.json1234567891011121314&#123; \"bootstrap\": false, \"server\": true, \"datacenter\": \"dc1\", \"node_name\": \"agent_two\", \"bind_addr\": \"172.20.20.11\", \"client_addr\": \"172.20.20.11\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"6MmrecCBuJMePdP2tOBDxw==\", \"log_level\": \"INFO\", \"enable_syslog\": true, \"enable_script_checks\": true, \"start_join\": [\"172.20.20.11\"]&#125; n3 config: /etc/consul.d/client/config.json12345678910111213&#123; \"server\": false, \"datacenter\": \"dc1\", \"node_name\": \"agent_three\", \"bind_addr\": \"172.20.20.12\", \"client_addr\": \"172.20.20.12\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"6MmrecCBuJMePdP2tOBDxw==\", \"log_level\": \"INFO\", \"enable_syslog\": true, \"enable_script_checks\": true, \"start_join\": [\"172.20.20.10\", \"172.20.20.11\"]&#125; 启动consul服务n1:1consul agent -config-dir=/etc/consul.d/bootstrap/ n2:1consul agent -config-dir=/etc/consul.d/server/ n3:1consul agent -config-dir=/etc/consul.d/client/ 验证使用vagrant登陆任意一个虚拟机：vagrant ssh n3 验证节点信息 12345vagrant@n3:~$ consul members -http-addr=172.20.20.11:8500Node Address Status Type Build Protocol DC Segmentageng_one 172.20.20.10:8301 alive server 1.0.1 2 dc1 &lt;all&gt;agent_two 172.20.20.11:8301 alive server 1.0.1 2 dc1 &lt;all&gt;agent_three 172.20.20.12:8301 alive client 1.0.1 2 dc1 &lt;default&gt; 注册服务 1234567891011121314151617181920curl --request PUT --data @payload.json http://172.20.20.11:8500/v1/agent/service/registerpayload.json:&#123; &quot;ID&quot;: &quot;redis2&quot;, &quot;Name&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;172.20.20.11&quot;, &quot;Port&quot;: 8000, &quot;EnableTagOverride&quot;: false, &quot;Check&quot;: &#123; &quot;DeregisterCriticalServiceAfter&quot;: &quot;90m&quot;, &quot;Args&quot;: [&quot;/usr/local/bin/check_redis.py&quot;], &quot;HTTP&quot;: &quot;http://www.baidu.com&quot;, &quot;Interval&quot;: &quot;10s&quot; &#125;&#125; 查询服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849vagrant@n3:~$ curl http://172.20.20.12:8500/v1/health/service/redis?pretty[ &#123; &quot;Node&quot;: &#123; &quot;ID&quot;: &quot;73653e92-b66b-04c6-40d5-0339226c9797&quot;, &quot;Node&quot;: &quot;agent_two&quot;, &quot;Address&quot;: &quot;172.20.20.11&quot;, &quot;Datacenter&quot;: &quot;dc1&quot;, &quot;TaggedAddresses&quot;: &#123; &quot;lan&quot;: &quot;172.20.20.11&quot;, &quot;wan&quot;: &quot;172.20.20.11&quot; &#125;, &quot;Meta&quot;: &#123; &quot;consul-network-segment&quot;: &quot;&quot; &#125;, &quot;CreateIndex&quot;: 691, &quot;ModifyIndex&quot;: 692 &#125;, &quot;Service&quot;: &#123; &quot;ID&quot;: &quot;redis2&quot;, &quot;Service&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;172.20.20.11&quot;, &quot;Port&quot;: 8000, &quot;EnableTagOverride&quot;: false, &quot;CreateIndex&quot;: 692, &quot;ModifyIndex&quot;: 692 &#125;, &quot;Checks&quot;: [ &#123; &quot;Node&quot;: &quot;agent_two&quot;, &quot;CheckID&quot;: &quot;serfHealth&quot;, &quot;Name&quot;: &quot;Serf Health Status&quot;, &quot;Status&quot;: &quot;passing&quot;, &quot;Notes&quot;: &quot;&quot;, &quot;Output&quot;: &quot;Agent alive and reachable&quot;, &quot;ServiceID&quot;: &quot;&quot;, &quot;ServiceName&quot;: &quot;&quot;, &quot;ServiceTags&quot;: [], &quot;Definition&quot;: &#123;&#125;, &quot;CreateIndex&quot;: 691, &quot;ModifyIndex&quot;: 691 &#125; ] &#125; ] 注意这个是查询全局服务列表，如果使用 curl http://172.20.20.12:8500/v1/agent/services 进行查询，只能查询当前agent管理的services","categories":[],"tags":[]},{"title":"linux mysql","slug":"linux-mysql","date":"2017-12-04T07:39:38.000Z","updated":"2017-12-04T07:41:52.642Z","comments":true,"path":"2017/12/04/linux-mysql/","link":"","permalink":"http://yoursite.com/2017/12/04/linux-mysql/","excerpt":"","text":"How to start or stop mysql on macosThere are different cases depending on whether you installed MySQL with the official binary installer, using MacPorts, or using Homebrew: MacPorts sudo launchctl unload -w /Library/LaunchDaemons/org.macports.mysql.plistsudo launchctl load -w /Library/LaunchDaemons/org.macports.mysql.plistNote: this is persistent after reboot. Homebrew launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plistlaunchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist Binary installer sudo /Library/StartupItems/MySQLCOM/MySQLCOM stopsudo /Library/StartupItems/MySQLCOM/MySQLCOM startsudo /Library/StartupItems/MySQLCOM/MySQLCOM restart in my mac os, the command is : sudo launchctl unload -w /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist","categories":[],"tags":[]},{"title":"spring mvc","slug":"spring-mvc","date":"2017-11-30T13:00:22.000Z","updated":"2017-12-01T02:02:30.110Z","comments":true,"path":"2017/11/30/spring-mvc/","link":"","permalink":"http://yoursite.com/2017/11/30/spring-mvc/","excerpt":"","text":"创建Spring Mvc项目 使用Intellij ide，创建一个空项目； 添加一个module，maven simple app模版； 增加spring framework支持； 最终project struct如下： 代码列表web.xml1234567891011121314151617181920212223242526272829&lt;!DOCTYPE web-app PUBLIC \"-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\" \"http://java.sun.com/dtd/web-app_2_3.dtd\" &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:dispatcher-servlet.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:dispatcher-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 其中 context-param 中指定了配置加载的路径； dispatcher-servlet.xml123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;context:component-scan base-package=\"controller\"/&gt;&lt;/beans&gt; 其中注意 xsi:schemaLocation，context:component-scan依赖与 http://www.springframework.org/schema/context/spring-context.xsd如果不添加，会报无法解析xml的错误； MainController1234567891011121314151617181920package controller;/** * Created by tianyuan on 2017/11/30. */import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(value = &quot;/&quot;)public class MainController &#123; @RequestMapping(method = RequestMethod.GET) @ResponseBody public String welcome() &#123; return &quot;hello world main controller&quot;; &#125;&#125; spring mvc 原理分析tomcat 在启动时会解析 web.xml 文件，web.xml就是入口； web.xml 中的 listener： org.springframework.web.context.ContextLoaderListener 会加载beans，解析annotation等； 其中context-param中的classpath，指classes文件夹路径，可以从编译后的target目录可以看出，编译和部署后的文件目录信息；","categories":[],"tags":[]},{"title":"create blog","slug":"create-blog","date":"2017-11-29T02:01:32.000Z","updated":"2017-11-29T02:35:16.176Z","comments":true,"path":"2017/11/29/create-blog/","link":"","permalink":"http://yoursite.com/2017/11/29/create-blog/","excerpt":"","text":"How to create your blog in three step first should install nodejs/git install hexo, https://hexo.io/ use theme next, http://theme-next.iissnan.com/ start your blog the blog is look like","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2017-11-29T01:50:32.274Z","updated":"2017-11-29T01:50:32.274Z","comments":true,"path":"2017/11/29/hello-world/","link":"","permalink":"http://yoursite.com/2017/11/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}