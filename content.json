{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com"},"pages":[{"title":"Repositories","date":"2017-12-11T09:16:13.809Z","updated":"2017-12-11T09:16:13.809Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"kafka design","slug":"kafka-design","date":"2017-12-11T06:45:40.000Z","updated":"2017-12-11T08:42:14.279Z","comments":true,"path":"2017/12/11/kafka-design/","link":"","permalink":"http://yoursite.com/2017/12/11/kafka-design/","excerpt":"","text":"转译：Varnish 架构师笔记 找到这篇文章是在阅读 Kafka 文档时，一个名为 “Don’t fear the filesystem!”的段落中提到的。文档指出，我们总是思维定势地以为磁盘很慢，内存很快。然而今天的计算机体系结构中，并非这么简单： 因为操作系统 PageCache 的存在，磁盘操作可能很快 虽然磁盘 IOPS 难以提高，但吞吐量在不断上升；换句话说，顺序读写磁盘非常快 CPU Cache 常常被忽略了，了解 CPU Cache 对提升内存读写性能至关重要 原文链接 当你开始深入 Varnish 的源代码后，应该会发觉它与你日常所见的一般应用软件有着明显不同，而这绝非偶然。 多年以来我的绝大部分时间花费在 FreeBSD 的内核开发上，而每每涉足用户空间编程，却总是毫无例外地发现那里的人们还在以1975年的方式工作。 所以 Varnish 这个项目一开始并没能激起我很大的兴趣。但我渐渐意识到 Varnish 虽然是一个用户态应用，但却也是一个能充分发挥我长久以来积累的关于硬件和内核的经验知识的理想场所。目前 Varnish 的开发已经进展到alpha版本的阶段，而我觉得应当承认自己相当享受这一段历程。 1975的编程方式到底出了什么问题？简而言之：计算机系统再也不应该被看作是由两个存储层次构成的了。 首先是主存：从水银延迟线，磁芯存储器，到现在可供随机访问的RAM。 然后是辅存：从纸带，磁带，到磁盘。最早的磁盘有一屋子大，然后缩小到洗衣机的尺寸，到今天硬盘可以被放进随身携带的 MP3 播放器中。 于是大家就按照这样的划分，在内存中分配变量，在磁盘中存取数据，以这样的方式进行编程工作。 还是拿 Squid 这个1975年风格的系统为例：你事先配置它的内存和硬盘用量，然后它会花费大把时间来追踪哪些HTTP对象驻留内存中，哪些存放在硬盘上，并且根据不同情况来调整它们的放置策略。 然而实际上呢，现今的计算机应被视为只使用一种统一的存储系统，这个系统完全基于硬盘（磁盘，固态硬盘或者其他什么东西），而传统的内存呢，在操作系统内核和虚拟内存硬件机制的帮助下可以被看作是硬盘的缓存。 回过头来看 Squid 的策略，它精心设计的存储管理机制实际上却陷入了与操作系统内核同样精巧的管理策略的激烈冲突。而就像所有内战一样，这样的冲突必然一事无成。 我们可以尝试从细节角度来看整个流程：一开始 Squid 会请求内存用来创建了一个 HTTP 对象，它往往会在创建之初被频繁访问多次，然后闲置一段时间。而后当内核接收到其他内存分配请求时，会将这些它认为闲置的内存数据放到硬盘交换分区去，而把这些回收的内存页用于更活跃的任务。在Squid下一次访问这一对象时，操作系统又会把暂存在交换分区的数据取回来供它使用。 这些内核对于内存的操作对于 Squid 是透明的，在它看来这个 HTTP 对象就像从没离开过内存一样，而实际上物理内存页得到了更有效的使用。 这就是虚拟内存机制。 如果事情到此为止的话就好了，但接下来1975年式的编程风格就出现了。 一段时间之后，Squid 也和内核一样注意到了这个对象闲置了，于是打算把它放到硬盘上去，省出一些内存来给更频繁访问的数据使用。所以它打开一个文件把这个对象写了进去。 打开慢镜头来看这个写入的过程： Squid 通过系统调用 write 将 HTTP 对象的虚拟内存地址传递给内核。由于物理页已经被内核交换出去，这个内存访问将引发一个缺页中断。为了重新载入这个内存页，内核不得不交换出另一个正在使用的内存页（很可能包含另一 Squid 的对象），修复页表，然后继续执行系统调用。Squid 对这些一无所知，它自以为只是进行了一次再普通不过的访存操作而已。 接下来 Squid 可以终于拿这块内存放别的数据了。而当这个对象再次被访问到时，Squid 又不得不把它从硬盘中取回来。首先它需要空闲的内存来存放这个对象，于是它根据某种淘汰算法选中另一个最近不常用的对象，把它写到硬盘上去（上面那些步骤重演了一遍）。然后打开文件读出这次所需的那个对象，最后通过网络套接字发送出去。 这一切显然充满了各种无用功。 让我们看看 Varnish 是怎么做的 Varnish 会直接请求大块虚拟内存，并由操作系统将这个内存空间映射到一个硬盘文件。当需要访问某个 HTTP 对象时，只需要正确索引这个对象相应的虚拟内存地址，剩下的交给操作系统就好了。当内核需要回收一些内存页时，它会自行决定将一些 Varnish 的内存数据写回到映射的文件中。而当 Varnish 再次访问这一块虚拟内存时，内核自然会腾出内存页来将文件中的数据读回使用。 仅此而已。 Varnish 不去尝试控制哪些数据应该被缓存在内存中，哪些应该放到硬盘上去。内核代码和硬件会处理这些事情，而且处理得很漂亮。 此外，与 Squid 不同的是 Varnish 只需要一个文件而不是为每个 HTTP 对象创建单独的文件。没有任何理由需要在 HTTP 对象和文件系统对象间建立一一对应的关系，也没有理由把时间浪费在文件系统的命名空间处理上。Varnish 需要处理的只是虚拟内存的指针和所需对象的长度值而已。 虚拟内存的出现为数据大于物理内存的场景提供了一种便利的机制，但人们似乎并没有领悟这一点。 更多的缓存CPU 的时钟频率目前看来基本止步于4GHz了。即便如此，为了避免内存读写的瓶颈，硬件工程师们不得不使用使用一级，二级，有时候甚至是三级 CPU cache（现在我们可以认为 RAM 是第四级缓存了），此外还有写缓冲，流水线，页模式读取等各种技术，而这些都是为了加快访存来匹配CPU的处理速度。 虽然时钟频率方面受限，但硅工艺的进步缩小了器件尺寸，集成了更多的晶体管。所以多核 CPU 的设计逐渐成为主流，但这从编程模型角度看来实在是很糟糕的一件事。 虽然多核系统存在已久，但编写能够利用上多个 CPU 的代码依然是一件棘手的事。而要在多核系统上写出高性能的程序就更是如此了。 比如我们有两个计数器： 12unsigned n_foo;unsigned n_bar; 在一个 CPU 上执行了 n_foo++ 的操作会使得CPU读取 n_foo 的值然后写回。 读取一个内存位置首先会检查它是否在 CPU L1 cache 中命中，这挺难的除非它刚刚被使用过。接下来是 L2 cache，我们不妨假设依然没有命中吧。 如果是在一个单核系统上，CPU 会去内存读取数据然后就完事。但在多核系统中，我们必须去检查其他CPU核心是否缓存并修改了 n_foo 的数值。我们会发起一个特殊的总线事务做这种检查，如果其他 CPU 答复说它确实持有这样一份拷贝，它就需要将它写回到内存中。如果硬件设计良好，我们可能可以通过总线监听获得这份新数据，不然的话就需要去内存里读取它。 我们终于可以修改 n_foo 的值了，但其实修改完了之后一般不会直接将它写回到内存中。为了之后操作的快速存取，很可能我们会把它缓存起来。 现在假设另一个 CPU 需要执行 n_bar++ 的操作，它能够直接进行吗？答案是否定的。因为缓存的单位并不是字节而是 cache-line（典型值是 8-128 个字节）。所以当第一个 CPU 忙着处理 n_foo 时，第二个 CPU 必须等待，因为虽然它想获取的是另一个变量，但却不幸落在了同一个 cache-line 中。 明白了吧？没错，这有点丑。 我们该怎么办尽一切可能，减少内存操作。 下面是 Varnish 的一些做法。 当需要处理一个 HTTP 请求或响应时，我们会持有一组指针和一个内存工作区。我们并不需要在处理每个 HTTP 报头时都调用 malloc，而是一次性分配整个工作区的内存，然后按需从中获取所需空间。而当我们一次性释放全部报头时，只要将指针重置到工作区的起始位置即可。 当需要将 HTTP 报头从一个请求拷贝到另一个请求（或从从一个响应复制到另一个响应）时，并不需要进行字符串拷贝，而只要拷贝指针。如果源报头在这个过程中不会被不释放或改写，这样做是非常安全的。比如从客户端请求到后台请求的拷贝就是这样一个例子。 但在一些新构建的报头生命周期长于源报头的场景中，就需要另外分配内存了。例如当我们会缓存新 HTTP 对象时，首先就计算整个报头所需空间，然后通过一次 malloc 调用来获取内存。 另外我们会尽可能重用那些正被缓存的内存数据。 比如 Varnish 的 worker 线程是以最近最忙的方式调度的，也即是说一个 worker 线程空闲后会被放回到队列的最前端，使得它更有机会马上去处理下一个新请求，这样它的数据，栈空间和变量等很可能可以在 CPU 缓存中被重用，而不是再次从RAM中读取。 同时对于 worker 线程经常使用的数据，我们会把它们分配在每个线程的栈变量中，并且确保它们占据完整的内存页。这样我们就可以尽可能避免 cache-line 的竞争。 如果对你来说这些听起来都很陌生，我可以告诉你它们是确实有效的：Varnish 处理一个命中缓存的请求最多只需18个系统调用，而且其中不少只是为了获得时间戳来满足统计的需要。 这些技术并不新鲜，我们已经在内核开发中使用了10多年，现在该轮到你们来学习了:-) 如此，欢迎进入 Varnish，一个 2006风格架构的程序。 Linux cache reference : https://www.ibm.com/developerworks/cn/linux/l-cache/index.html","categories":[],"tags":[]},{"title":"kafka","slug":"kafka","date":"2017-12-11T03:16:36.000Z","updated":"2017-12-11T03:31:34.102Z","comments":true,"path":"2017/12/11/kafka/","link":"","permalink":"http://yoursite.com/2017/12/11/kafka/","excerpt":"","text":"kafka internalkafka operator list kafka brokers 123456789tianyuan# ./bin/zookeeper-shell.sh localhost:2181/kafpush &lt;&lt;&lt; &quot;ls /brokers/ids&quot;Connecting to localhost:2181/kafpushWelcome to ZooKeeper!JLine support is disabledWATCHER::WatchedEvent state:SyncConnected type:None path:null[3, 2, 1] 注意： ./bin/zookeeper-shell.sh localhost:2181 会连接到kafka所在的zk集群 create topic 1./bin/kafka-topics.sh --create --zookeeper localhost:2181/kafpush --replication-factor 1 --partitions 300 --topic test alter topic partitions 1./bin/kafka-topics.sh --alter --zookeeper localhost:2181/kafpush --partitions 300 --topic test","categories":[],"tags":[]},{"title":"pipework demo","slug":"pipework-demo","date":"2017-12-07T12:58:07.000Z","updated":"2017-12-08T10:57:01.009Z","comments":true,"path":"2017/12/07/pipework-demo/","link":"","permalink":"http://yoursite.com/2017/12/07/pipework-demo/","excerpt":"","text":"pipework demo主机A地址为10.10.101.105/24,网关为10.10.101.254,需要给Docker容器的地址配置为10.10.101.150/24。在主机A上做如下操作： 12345678910111213141516#安装pipeworkgit clone https://github.com/jpetazzo/pipeworkcp ~/pipework/pipework /usr/local/bin/#启动Docker容器。docker run -itd --name test1 ubuntu /bin/bash#配置容器网络，并连到网桥br0上。网关在IP地址后面加@指定。#若主机环境中存在dhcp服务器，也可以通过dhcp的方式获取IP#pipework br0 test1 dhcppipework br0 test1 10.10.101.150/24@10.10.101.254#将主机eth0桥接到br0上，并把eth0的IP配置在br0上。这里由于是远程操作，中间网络会断掉，所以放在一条命令中执行。#将主机eth0桥接到br0上，eth0就成了虚拟网桥br0的一部分。ip addr add 10.10.101.105/24 dev br0; \\ ip addr del 10.10.101.105/24 dev eth0; \\ brctl addif br0 eth0; \\ ip route del default; \\ ip route add default gw 10.10.101.254 dev br0 12345678910111213141516171819202122232425#创建br0网桥#若ovs开头，则创建OVS网桥 ovs-vsctl add-br ovs*#IFNAME=br0brctl addbr $IFNAME#创建veth pair,用于连接容器和br0#LOCAL_IFNAME=eh0 GUEST_IFNAME=eth1ip link add name $LOCAL_IFNAME mtu $MTU type veth peer name $GUEST_IFNAME mtu $MTU#找到Docker容器test1在主机上的PID,创建容器网络命名空间的软连接#GUESTNAME=test1DOCKERPID=$(docker inspect --format=&apos;&#123;&#123; .State.Pid &#125;&#125;&apos; $GUESTNAME)ln -s /proc/$NSPID/ns/net /var/run/netns/$NSPID#将veth pair一端放入Docker容器中，并设置正确的名字eth1#CONTAINER_IFNAME=eth1#把$GUEST_IFNAME添加到$NSPID网络命名空间中，也就是虚拟网络环境ip link set $GUEST_IFNAME netns $NSPIDip netns exec $NSPID ip link set $GUEST_IFNAME name $CONTAINER_IFNAME#将veth pair另一端加入网桥#若为OVS网桥则为 ovs-vsctl add-port $IFNAME $LOCAL_IFNAME $&#123;VLAN:+&quot;tag=$VLAN&quot;&#125;brctl addif $IFNAME $LOCAL_IFNAME#为新增加的容器配置IP和路由#GATEWAY=10.10.101.254ip netns exec $NSPID ip addr add $IPADDR dev $CONTAINER_IFNAMEip netns exec $NSPID ip link set $CONTAINER_IFNAME upip netns exec $NSPID ip route delete defaultip netns exec $NSPID ip route add $GATEWAY/32 dev $CONTAINER_IFNAME 首先pipework检查是否存在br0网桥，若不存在，就自己创建。若以”ovs”开头，就会创建OpenVswitch网桥，以”br”开头，创建Linux bridge。 创建veth pair设备，用于为容器提供网卡并连接到br0网桥。 使用docker inspect找到容器在主机中的PID，然后通过PID将容器的网络命名空间链接到/var/run/netns/目录下。这么做的目的是，方便在主机上使用ip netns命令配置容器的网络。因为，在Docker容器中，我们没有权限配置网络环境。 将之前创建的veth pair设备分别加入容器和网桥中。在容器中的名称默认为eth1，可以通过pipework的-i参数修改该名称。 然后就是配置新网卡的IP。若在IP地址的后面加上网关地址，那么pipework会重新配置默认路由。这样容器通往外网的流量会经由新配置的eth1出去，而不是通过eth0和docker0。(若想完全抛弃自带的网络设置，在启动容器的时候可以指定–net=none) ip netns相关命令 增加虚拟网络命名空间 1ip netns add net0 显示所有的虚拟网络命名空间 123EULER:~ # ip netns listnet0 也可通过查看/var/run/netns目录下的文件来list 123EULER:~ # ls /var/run/netns/net0 进入虚拟机网络环境 12345678910111213ip netns exec net0 `command` 如EULER:~ # ip netns exec net0 bash #打开虚拟网络环境net0的bash窗口EULER:~ # ip addr #显示所有虚拟网络环境的设备EULER:~ # exit #退出该网络虚拟环境exit 增加一对veth虚拟网卡 1EULER:~ # ip link add type veth 将veth0添加到net0虚拟网络环境 1ip link set veth0 netns net0 将虚拟网卡veth1改名并添加到net1虚拟网络环境中 1ip link set dev veth1 name net1-bridge netns net1 设置虚拟网络环境net0的veth0设备处于激活状态 1ip netns exec net0 ip link set veth0 up 为虚拟网络环境net0的veth0设备增加IP地址 1ip netns exec net0 ip address add 10.0.1.1/24 dev veth0 在操作过程中遇到的问题 添加路由时的错误 12ip route add default gw 10.211.55.1 dev eth0Error: either &quot;to&quot; is duplicate, or &quot;gw&quot; is a garbage. 需要修改成： ip route add 10.221.55.0/24 via 10.211.55.1 dev eth0","categories":[],"tags":[]},{"title":"ansible 通过跳板机管理远程服务器","slug":"ansible-通过跳板机管理远程服务器","date":"2017-12-07T01:52:19.000Z","updated":"2017-12-07T02:09:40.629Z","comments":true,"path":"2017/12/07/ansible-通过跳板机管理远程服务器/","link":"","permalink":"http://yoursite.com/2017/12/07/ansible-通过跳板机管理远程服务器/","excerpt":"","text":"环境信息 mac笔记本 跳板机：jump.example.com 被管理节点：node1.example.com, node2.example.com, node3.example.com mac 笔记本无法直接连接被管理节点，需要通过跳板机进行管理； 修改ssh config123456Host node1.example.com node2.example.com node3.example.com ForwardAgent yes User user Port 22 IdentityFile ~/.ssh/net_id_rsa ProxyCommand ssh -qaY -i ~/.ssh/net_id_rsa -p 1046 user@jump.example.com &apos;nc -w 14400 %h %p&apos; 注意：我这边从跳板机到被管理节点和mac到跳板机的私钥是一样的，都是 net_id_rsa 如果不一样，需要修改 IdentityFile 和 ProxyCommand 的 -i 参数 IdentityFile是跳板机登陆被管理节点使用的私钥文件，ProxyCommand -i 参数使用的私钥是mac登陆跳板机所使用的私钥 修改ansible inventory12345host file:[node]node1 ansible_host=node1.example.comnode2 ansible_host=node2.example.comnode3 ansible_host=node3.example.com 验证1ansible -i host -m ping","categories":[],"tags":[]},{"title":"consul deploy","slug":"consul-deploy","date":"2017-12-05T08:21:16.000Z","updated":"2017-12-05T08:37:42.461Z","comments":true,"path":"2017/12/05/consul-deploy/","link":"","permalink":"http://yoursite.com/2017/12/05/consul-deploy/","excerpt":"","text":"consul 集群部署从consul github clone 工程~/src/github.com/hashicorp/consul 修改 Vagrantfile 文件~/src/github.com/hashicorp/consul/demo/vagrant-cluster/Vagrantfile 在最后增加: 1234config.vm.define &quot;n3&quot; do |n3| n3.vm.hostname = &quot;n3&quot; n3.vm.network &quot;private_network&quot;, ip: &quot;172.20.20.12&quot;end 启动虚拟机使用vagrant up启动虚拟机 部署信息 HostName IP Address Consul Role n1 172.20.20.10 bootstrap consul server n2 172.20.20.11 server n3 172.20.20.12 client n1 config: /etc/consul.d/bootstrap/config.json12345678910111213&#123; \"bootstrap\": true, \"server\": true, \"node_name\": \"ageng_one\", \"bind_addr\": \"172.20.20.10\", \"client_addr\": \"172.20.20.10\", \"datacenter\": \"dc1\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"6MmrecCBuJMePdP2tOBDxw==\", \"log_level\": \"INFO\", \"enable_script_checks\": true, \"enable_syslog\": true&#125; n2 config: /etc/consul.d/server/config.json1234567891011121314&#123; \"bootstrap\": false, \"server\": true, \"datacenter\": \"dc1\", \"node_name\": \"agent_two\", \"bind_addr\": \"172.20.20.11\", \"client_addr\": \"172.20.20.11\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"6MmrecCBuJMePdP2tOBDxw==\", \"log_level\": \"INFO\", \"enable_syslog\": true, \"enable_script_checks\": true, \"start_join\": [\"172.20.20.11\"]&#125; n3 config: /etc/consul.d/client/config.json12345678910111213&#123; \"server\": false, \"datacenter\": \"dc1\", \"node_name\": \"agent_three\", \"bind_addr\": \"172.20.20.12\", \"client_addr\": \"172.20.20.12\", \"data_dir\": \"/tmp/consul\", \"encrypt\": \"6MmrecCBuJMePdP2tOBDxw==\", \"log_level\": \"INFO\", \"enable_syslog\": true, \"enable_script_checks\": true, \"start_join\": [\"172.20.20.10\", \"172.20.20.11\"]&#125; 启动consul服务n1:1consul agent -config-dir=/etc/consul.d/bootstrap/ n2:1consul agent -config-dir=/etc/consul.d/server/ n3:1consul agent -config-dir=/etc/consul.d/client/ 验证使用vagrant登陆任意一个虚拟机：vagrant ssh n3 验证节点信息 12345vagrant@n3:~$ consul members -http-addr=172.20.20.11:8500Node Address Status Type Build Protocol DC Segmentageng_one 172.20.20.10:8301 alive server 1.0.1 2 dc1 &lt;all&gt;agent_two 172.20.20.11:8301 alive server 1.0.1 2 dc1 &lt;all&gt;agent_three 172.20.20.12:8301 alive client 1.0.1 2 dc1 &lt;default&gt; 注册服务 1234567891011121314151617181920curl --request PUT --data @payload.json http://172.20.20.11:8500/v1/agent/service/registerpayload.json:&#123; &quot;ID&quot;: &quot;redis2&quot;, &quot;Name&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;172.20.20.11&quot;, &quot;Port&quot;: 8000, &quot;EnableTagOverride&quot;: false, &quot;Check&quot;: &#123; &quot;DeregisterCriticalServiceAfter&quot;: &quot;90m&quot;, &quot;Args&quot;: [&quot;/usr/local/bin/check_redis.py&quot;], &quot;HTTP&quot;: &quot;http://www.baidu.com&quot;, &quot;Interval&quot;: &quot;10s&quot; &#125;&#125; 查询服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849vagrant@n3:~$ curl http://172.20.20.12:8500/v1/health/service/redis?pretty[ &#123; &quot;Node&quot;: &#123; &quot;ID&quot;: &quot;73653e92-b66b-04c6-40d5-0339226c9797&quot;, &quot;Node&quot;: &quot;agent_two&quot;, &quot;Address&quot;: &quot;172.20.20.11&quot;, &quot;Datacenter&quot;: &quot;dc1&quot;, &quot;TaggedAddresses&quot;: &#123; &quot;lan&quot;: &quot;172.20.20.11&quot;, &quot;wan&quot;: &quot;172.20.20.11&quot; &#125;, &quot;Meta&quot;: &#123; &quot;consul-network-segment&quot;: &quot;&quot; &#125;, &quot;CreateIndex&quot;: 691, &quot;ModifyIndex&quot;: 692 &#125;, &quot;Service&quot;: &#123; &quot;ID&quot;: &quot;redis2&quot;, &quot;Service&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;172.20.20.11&quot;, &quot;Port&quot;: 8000, &quot;EnableTagOverride&quot;: false, &quot;CreateIndex&quot;: 692, &quot;ModifyIndex&quot;: 692 &#125;, &quot;Checks&quot;: [ &#123; &quot;Node&quot;: &quot;agent_two&quot;, &quot;CheckID&quot;: &quot;serfHealth&quot;, &quot;Name&quot;: &quot;Serf Health Status&quot;, &quot;Status&quot;: &quot;passing&quot;, &quot;Notes&quot;: &quot;&quot;, &quot;Output&quot;: &quot;Agent alive and reachable&quot;, &quot;ServiceID&quot;: &quot;&quot;, &quot;ServiceName&quot;: &quot;&quot;, &quot;ServiceTags&quot;: [], &quot;Definition&quot;: &#123;&#125;, &quot;CreateIndex&quot;: 691, &quot;ModifyIndex&quot;: 691 &#125; ] &#125; ] 注意这个是查询全局服务列表，如果使用 curl http://172.20.20.12:8500/v1/agent/services 进行查询，只能查询当前agent管理的services","categories":[],"tags":[]},{"title":"linux mysql","slug":"linux-mysql","date":"2017-12-04T07:39:38.000Z","updated":"2017-12-04T07:41:52.642Z","comments":true,"path":"2017/12/04/linux-mysql/","link":"","permalink":"http://yoursite.com/2017/12/04/linux-mysql/","excerpt":"","text":"How to start or stop mysql on macosThere are different cases depending on whether you installed MySQL with the official binary installer, using MacPorts, or using Homebrew: MacPorts sudo launchctl unload -w /Library/LaunchDaemons/org.macports.mysql.plistsudo launchctl load -w /Library/LaunchDaemons/org.macports.mysql.plistNote: this is persistent after reboot. Homebrew launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plistlaunchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist Binary installer sudo /Library/StartupItems/MySQLCOM/MySQLCOM stopsudo /Library/StartupItems/MySQLCOM/MySQLCOM startsudo /Library/StartupItems/MySQLCOM/MySQLCOM restart in my mac os, the command is : sudo launchctl unload -w /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist","categories":[],"tags":[]},{"title":"spring mvc","slug":"spring-mvc","date":"2017-11-30T13:00:22.000Z","updated":"2017-12-01T02:02:30.110Z","comments":true,"path":"2017/11/30/spring-mvc/","link":"","permalink":"http://yoursite.com/2017/11/30/spring-mvc/","excerpt":"","text":"创建Spring Mvc项目 使用Intellij ide，创建一个空项目； 添加一个module，maven simple app模版； 增加spring framework支持； 最终project struct如下： 代码列表web.xml1234567891011121314151617181920212223242526272829&lt;!DOCTYPE web-app PUBLIC \"-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\" \"http://java.sun.com/dtd/web-app_2_3.dtd\" &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:dispatcher-servlet.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:dispatcher-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 其中 context-param 中指定了配置加载的路径； dispatcher-servlet.xml123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;context:component-scan base-package=\"controller\"/&gt;&lt;/beans&gt; 其中注意 xsi:schemaLocation，context:component-scan依赖与 http://www.springframework.org/schema/context/spring-context.xsd如果不添加，会报无法解析xml的错误； MainController1234567891011121314151617181920package controller;/** * Created by tianyuan on 2017/11/30. */import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(value = &quot;/&quot;)public class MainController &#123; @RequestMapping(method = RequestMethod.GET) @ResponseBody public String welcome() &#123; return &quot;hello world main controller&quot;; &#125;&#125; spring mvc 原理分析tomcat 在启动时会解析 web.xml 文件，web.xml就是入口； web.xml 中的 listener： org.springframework.web.context.ContextLoaderListener 会加载beans，解析annotation等； 其中context-param中的classpath，指classes文件夹路径，可以从编译后的target目录可以看出，编译和部署后的文件目录信息；","categories":[],"tags":[]},{"title":"create blog","slug":"create-blog","date":"2017-11-29T02:01:32.000Z","updated":"2017-11-29T02:35:16.176Z","comments":true,"path":"2017/11/29/create-blog/","link":"","permalink":"http://yoursite.com/2017/11/29/create-blog/","excerpt":"","text":"How to create your blog in three step first should install nodejs/git install hexo, https://hexo.io/ use theme next, http://theme-next.iissnan.com/ start your blog the blog is look like","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2017-11-29T01:50:32.274Z","updated":"2017-11-29T01:50:32.274Z","comments":true,"path":"2017/11/29/hello-world/","link":"","permalink":"http://yoursite.com/2017/11/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}